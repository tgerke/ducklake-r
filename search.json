[{"path":"https://tgerke.github.io/ducklake-r/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 ducklake authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Clinical Trial Data Lake with ducklake","text":"Electronic Data Capture (EDC) systems provide built-logging audit trails collected clinical trial data, significant data management challenges emerge data exported EDC statistical programming analysis. SDTM datasets created analysis begins, traditional file-based approaches often result : Disconnected Flat Files: CDISC datasets inherently relational (linked USUBJID, domain keys) typically stored separate XPT/CSV files isolated SAS datasets, often requiring manual loading joining cross-domain analyses Loss Audit Trail: EDC protections disappear; tracking changes derived datasets becomes manual Version Control Challenges: Multiple versions analysis datasets scattered across folders drives Data Lineage Issues: Unclear provenance derived variables ADaM datasets Reproducibility Concerns: Inability recreate analyses specific time points Collaboration Friction: Multiple statistical programmers working different versions data Regulatory Gaps: Difficulty demonstrating 21 CFR Part 11 compliance derived datasets ducklake package addresses post-EDC challenges implementing versioned data lake architecture specifically designed statistical programming workflows R. Rather managing disconnected flat files, DuckLake provides modern relational database structure preserves inherent relationships CDISC datasets adding enterprise-grade version control. storing SDTM (Study Data Tabulation Model) ADaM (Analysis Data Model) datasets along regulatory submission artifacts (define.xml, ARD, ARM, specifications) DuckLake, statistical programmers gain: Relational Data Model: CDISC datasets inherently relational explicit keys (USUBJID, domain relationships) traditionally stored disconnected flat files (XPT, CSV). DuckLake preserves relational structure, enabling efficient joins queries across domains without loading multiple files Modern Data Architecture: Move file-based database-backed workflows maintaining R’s familiar data frame interface Automatic Versioning: Every data modification tracked timestamps metadata Time Travel: Query data existed point time Audit Trail: Complete history data changes regulatory compliance Reproducibility: Recreate analyses exactly run previously Collaboration: Multiple analysts can work safely shared data Performance: Fast queries large datasets using DuckDB’s columnar storage query optimization Transactions: Atomic updates ensure data consistency across related datasets Unified Storage: Keep datasets alongside regulatory artifacts (define.xml, ARD, ARM) one versioned repository vignette demonstrates set clinical trial data lake, starting SDTM domains building analysis-ready ADaM datasets, including storage regulatory submission artifacts.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"setting-up-the-data-lake","dir":"Articles","previous_headings":"","what":"Setting Up the Data Lake","title":"Clinical Trial Data Lake with ducklake","text":"First, ’ll create new DuckLake store clinical trial data. establishes foundational infrastructure versioned data repository. ’ll use temporary directory vignette, practice specify permanent location using lake_path argument (e.g., shared network drive project directory).","code":"# Install the ducklake extension to duckdb (required once per system) install_ducklake()  # Define where to create a new data lake or access an existing one # For this vignette, we use vignette_temp_dir; in practice, use a permanent location # lake_path <- \"/path/to/your/project/data_lake\" trial_lake_path <- vignette_temp_dir  # attach_ducklake creates or attaches (if it already exists) a data lake attach_ducklake(   ducklake_name = \"clinical_trial_lake\",   lake_path = trial_lake_path )  # Verify the lake was created list.files(trial_lake_path, pattern = \"clinical_trial_lake\") #> [1] \"clinical_trial_lake.ducklake\"     \"clinical_trial_lake.ducklake.wal\""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"data-lake-architecture-medallion-layers","dir":"Articles","previous_headings":"","what":"Data Lake Architecture: Medallion Layers","title":"Clinical Trial Data Lake with ducklake","text":"loading data, ’s important understand layered architecture ’ll use. follows medallion architecture pattern common modern data lakes: Bronze Layer (Raw): Data exactly received source systems, transformations. preserves original data audit trails reprocessing. Silver Layer (Cleaned): Standardized cleaned data transformations like convert_blanks_to_na(), type conversions, validation. trusted source deriving analysis datasets. Gold Layer (Analytics): Business-logic datasets optimized specific analyses, ADaM datasets. analysis happens. approach provides: Complete Audit Trail: Original data preserved alongside transformations Reprocessability: cleaning logic changes, reprocess bronze without re-extracting Data Lineage: Clear progression raw → cleaned → analysis-ready Validation: Compare layers verify transformations Regulatory Compliance: Demonstrate source data lost improperly altered","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"loading-sdtm-domains","dir":"Articles","previous_headings":"","what":"Loading SDTM Domains","title":"Clinical Trial Data Lake with ducklake","text":"SDTM datasets form foundation clinical trial data. ’ll load several key domains pharmaverse SDTM collection, contains realistic test data CDISC pilot study. domain, ’ll: 1. Load raw data bronze layer (received) 2. Apply cleaning transformations create silver layer (analysis-ready)","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"demographics-dm","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Demographics (DM)","title":"Clinical Trial Data Lake with ducklake","text":"Demographics domain contains baseline characteristics subject.","code":"# Bronze layer: Load raw SDTM Demographics exactly as received with_transaction(   create_table(pharmaversesdtm::dm, \"dm_raw\"),   author = \"T Gerke\",   commit_message = \"Add raw demographics\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Silver layer: Apply cleaning transformations with_transaction(   get_ducklake_table(\"dm_raw\") |>      admiral::convert_blanks_to_na() |>      create_table(\"dm\"),   author = \"T Gerke\",   commit_message = \"Clean demographics data\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Verify the cleaned table get_ducklake_table(\"dm\") |>    select(USUBJID, AGE, SEX, RACE, ARM) |>    head() #> # Source:   SQL [?? x 5] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>   USUBJID       AGE SEX   RACE  ARM                  #>   <chr>       <dbl> <chr> <chr> <chr>                #> 1 01-701-1015    63 F     WHITE Placebo              #> 2 01-701-1023    64 M     WHITE Placebo              #> 3 01-701-1028    71 M     WHITE Xanomeline High Dose #> 4 01-701-1033    74 M     WHITE Xanomeline Low Dose  #> 5 01-701-1034    77 F     WHITE Xanomeline High Dose #> 6 01-701-1047    85 F     WHITE Placebo"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"supplemental-demographics-suppdm","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Supplemental Demographics (SUPPDM)","title":"Clinical Trial Data Lake with ducklake","text":"Supplemental domains contain additional variables parent domain.","code":"# Bronze layer: Raw data with_transaction(   create_table(pharmaversesdtm::suppdm, \"suppdm_raw\"),   author = \"T Gerke\",   commit_message = \"Add raw supplemental demographics\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Silver layer: Cleaned data with_transaction(   get_ducklake_table(\"suppdm_raw\") |>      admiral::convert_blanks_to_na() |>      create_table(\"suppdm\"),   author = \"T Gerke\",   commit_message = \"Clean supplemental demographics\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"disposition-ds","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Disposition (DS)","title":"Clinical Trial Data Lake with ducklake","text":"Disposition domain tracks subject progress study.","code":"# Bronze layer with_transaction(   create_table(pharmaversesdtm::ds, \"ds_raw\"),   author = \"T Gerke\",   commit_message = \"Add raw disposition\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Silver layer with_transaction(   ds <- get_ducklake_table(\"ds_raw\") |>      admiral::convert_blanks_to_na() |>      create_table(\"ds\"),   author = \"T Gerke\",   commit_message = \"Clean disposition data\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"exposure-ex","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Exposure (EX)","title":"Clinical Trial Data Lake with ducklake","text":"Exposure domain contains treatment administration records.","code":"# Bronze layer with_transaction(   create_table(pharmaversesdtm::ex, \"ex_raw\"),   author = \"T Gerke\",   commit_message = \"Add raw exposure\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Silver layer with_transaction(   ex <- get_ducklake_table(\"ex_raw\") |>      admiral::convert_blanks_to_na() |>      create_table(\"ex\"),   author = \"T Gerke\",   commit_message = \"Clean exposure data\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"adverse-events-ae","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Adverse Events (AE)","title":"Clinical Trial Data Lake with ducklake","text":"Adverse Events domain records safety data.","code":"# Bronze layer with_transaction(   create_table(pharmaversesdtm::ae, \"ae_raw\"),   author = \"T Gerke\",   commit_message = \"Add raw adverse events\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Silver layer with_transaction(   ae <- get_ducklake_table(\"ae_raw\") |>      admiral::convert_blanks_to_na() |>      create_table(\"ae\"),   author = \"T Gerke\",   commit_message = \"Clean adverse events\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"vital-signs-vs","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Vital Signs (VS)","title":"Clinical Trial Data Lake with ducklake","text":"Vital Signs data used deriving baseline values.","code":"# Bronze layer with_transaction(   create_table(pharmaversesdtm::vs, \"vs_raw\"),   author = \"T Gerke\",   commit_message = \"Add raw vital signs\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Silver layer with_transaction(   vs <- get_ducklake_table(\"vs_raw\") |>      admiral::convert_blanks_to_na() |>      create_table(\"vs\"),   author = \"T Gerke\",   commit_message = \"Clean vital signs\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"pharmacokinetic-concentrations-pc","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Pharmacokinetic Concentrations (PC)","title":"Clinical Trial Data Lake with ducklake","text":"PK analysis, ’ll also load concentration data.","code":"# Bronze layer with_transaction(   create_table(pharmaversesdtm::pc, \"pc_raw\"),   author = \"T Gerke\",   commit_message = \"Add raw PK concentrations\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Silver layer with_transaction(   pc <- get_ducklake_table(\"pc_raw\") |>      admiral::convert_blanks_to_na() |>      create_table(\"pc\"),   author = \"T Gerke\",   commit_message = \"Clean PK concentrations\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"verifying-sdtm-version-control","dir":"Articles","previous_headings":"Loading SDTM Domains","what":"Verifying SDTM Version Control","title":"Clinical Trial Data Lake with ducklake","text":"Let’s verify SDTM data bronze silver layers indeed versioned. create_table() call within transaction automatically creates snapshot metadata. demonstrates : Every table creation versioned - bronze (dm_raw) silver (dm) layers version 1.0 Metadata captured - snapshot includes timestamp table information Time travel works - can retrieve specific version using get_ducklake_table_version() Audit trail exists - Complete history maintained regulatory compliance","code":"# View the first 5 of all snapshots in the data lake list_table_snapshots() |>   head(5) #>   snapshot_id       snapshot_time schema_version #> 1           0 2026-02-09 21:16:40              0 #> 2           1 2026-02-09 21:16:40              1 #> 3           2 2026-02-09 21:16:40              2 #> 4           3 2026-02-09 21:16:40              3 #> 5           4 2026-02-09 21:16:40              4 #>                                                    changes  author #> 1                                    schemas_created, main    <NA> #> 2     tables_created, tables_inserted_into, main.dm_raw, 1 T Gerke #> 3         tables_created, tables_inserted_into, main.dm, 2 T Gerke #> 4 tables_created, tables_inserted_into, main.suppdm_raw, 3 T Gerke #> 5     tables_created, tables_inserted_into, main.suppdm, 4 T Gerke #>                      commit_message commit_extra_info #> 1                              <NA>              <NA> #> 2              Add raw demographics              <NA> #> 3           Clean demographics data              <NA> #> 4 Add raw supplemental demographics              <NA> #> 5   Clean supplemental demographics              <NA>  # Filter snapshots for specific tables list_table_snapshots(\"dm_raw\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:16:40              1 #>                                                changes  author #> 2 tables_created, tables_inserted_into, main.dm_raw, 1 T Gerke #>         commit_message commit_extra_info #> 2 Add raw demographics              <NA> list_table_snapshots(\"dm\") #>   snapshot_id       snapshot_time schema_version #> 3           2 2026-02-09 21:16:40              2 #>                                            changes  author #> 3 tables_created, tables_inserted_into, main.dm, 2 T Gerke #>            commit_message commit_extra_info #> 3 Clean demographics data              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"building-the-analytics-layer-adam-datasets-gold-layer","dir":"Articles","previous_headings":"","what":"Building the Analytics Layer: ADaM Datasets (Gold Layer)","title":"Clinical Trial Data Lake with ducklake","text":"Now SDTM data loaded versioned silver layer, ’ll create analysis datasets following ADaM standards gold layer. datasets apply business logic derivations optimized specific analyses. dataset stored data lake full version control. gold layer reads silver layer (cleaned SDTM), ensuring analysis datasets built trusted, standardized source data.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"adsl-subject-level-analysis-dataset","dir":"Articles","previous_headings":"Building the Analytics Layer: ADaM Datasets (Gold Layer)","what":"ADSL: Subject-Level Analysis Dataset","title":"Clinical Trial Data Lake with ducklake","text":"ADSL fundamental ADaM dataset containing one record per subject key analysis variables.","code":"# Read SDTM data from the lake and collect into memory # Admiral functions require tibbles/data.frames, not lazy database connections dm <- get_ducklake_table(\"dm\") |> collect() suppdm <- get_ducklake_table(\"suppdm\") |> collect() ds <- get_ducklake_table(\"ds\") |> collect() ex <- get_ducklake_table(\"ex\") |> collect() ae <- get_ducklake_table(\"ae\") |> collect() vs <- get_ducklake_table(\"vs\") |> collect()  # Combine DM and SUPPDM dm_suppdm <- dm |>    left_join(     suppdm |>        filter(QNAM %in% c(\"EDUCLVL\", \"DISCONFL\", \"DSRAEFL\")) |>        pivot_wider(         id_cols = c(STUDYID, USUBJID),         names_from = QNAM,         values_from = QVAL       ),     by = c(\"STUDYID\", \"USUBJID\")   )  # Derive treatment dates and durations ex_ext <- ex |>    derive_vars_dtm(     dtc = EXSTDTC,     new_vars_prefix = \"EXST\"   ) |>    derive_vars_dtm(     dtc = EXENDTC,     new_vars_prefix = \"EXEN\",     time_imputation = \"last\"   )  # Derive disposition variables first (needed for later derivations) ds_ext <- ds |>    derive_vars_dt(     dtc = DSSTDTC,     new_vars_prefix = \"DSST\"   )  # Build ADSL with all derivations in a single pipeline adsl <- dm_suppdm |>    # Treatment Start Datetime   derive_vars_merged(     dataset_add = ex_ext,     filter_add = (EXDOSE > 0 | (EXDOSE == 0 & str_detect(EXTRT, \"PLACEBO\"))) &                   !is.na(EXSTDTM),     new_vars = exprs(TRTSDTM = EXSTDTM),     order = exprs(EXSTDTM, EXSEQ),     mode = \"first\",     by_vars = exprs(STUDYID, USUBJID)   ) |>    # Treatment End Datetime   derive_vars_merged(     dataset_add = ex_ext,     filter_add = (EXDOSE > 0 | (EXDOSE == 0 & str_detect(EXTRT, \"PLACEBO\"))) &                   !is.na(EXENDTM),     new_vars = exprs(TRTEDTM = EXENDTM),     order = exprs(EXENDTM, EXSEQ),     mode = \"last\",     by_vars = exprs(STUDYID, USUBJID)   ) |>    # Convert to dates   derive_vars_dtm_to_dt(source_vars = exprs(TRTSDTM, TRTEDTM)) |>    # Treatment duration   derive_var_trtdurd() |>    # Safety population flag   derive_var_merged_exist_flag(     dataset_add = ex,     by_vars = exprs(STUDYID, USUBJID),     new_var = SAFFL,     condition = (EXDOSE > 0 | (EXDOSE == 0 & str_detect(EXTRT, \"PLACEBO\")))   ) |>    # Treatment variables   mutate(     TRT01P = ARM,     TRT01A = ACTARM   ) |>    # Age groups   mutate(     AGEGR1 = case_when(       AGE < 18 ~ \"<18\",       between(AGE, 18, 64) ~ \"18-64\",       AGE > 64 ~ \">64\",       TRUE ~ \"Missing\"     ),     AGEGR1N = case_when(       AGE < 18 ~ 1,       between(AGE, 18, 64) ~ 2,       AGE > 64 ~ 3,       TRUE ~ 4     )   ) |>    # Randomization date   derive_vars_merged(     dataset_add = ds_ext,     by_vars = exprs(STUDYID, USUBJID),     new_vars = exprs(RANDDT = DSSTDT),     filter_add = DSDECOD == \"RANDOMIZED\"   ) |>    # End of study date   derive_vars_merged(     dataset_add = ds_ext,     by_vars = exprs(STUDYID, USUBJID),     new_vars = exprs(EOSDT = DSSTDT),     filter_add = DSCAT == \"DISPOSITION EVENT\" & DSDECOD != \"SCREEN FAILURE\"   ) |>    # End of study status   mutate(     EOSSTT = case_when(       is.na(EOSDT) ~ \"ONGOING\",       TRUE ~ \"COMPLETED\"     )   )  # Store ADSL in the data lake with_transaction(   create_table(adsl, \"adsl\"),   author = \"T Gerke\",   commit_message = \"Create ADSL dataset\",   commit_extra_info = \"Derived from DM, SUPPDM, DS, EX; includes treatment dates, safety flags, age groups\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Preview ADSL get_ducklake_table(\"adsl\") |>    select(USUBJID, AGE, AGEGR1, TRT01P, TRTSDT, TRTEDT, SAFFL) |>   head(10) #> # Source:   SQL [?? x 7] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>    USUBJID       AGE AGEGR1 TRT01P               TRTSDT     TRTEDT     SAFFL #>    <chr>       <dbl> <chr>  <chr>                <date>     <date>     <chr> #>  1 01-701-1015    63 18-64  Placebo              2014-01-02 2014-07-02 Y     #>  2 01-701-1023    64 18-64  Placebo              2012-08-05 2012-09-01 Y     #>  3 01-701-1028    71 >64    Xanomeline High Dose 2013-07-19 2014-01-14 Y     #>  4 01-701-1033    74 >64    Xanomeline Low Dose  2014-03-18 2014-03-31 Y     #>  5 01-701-1034    77 >64    Xanomeline High Dose 2014-07-01 2014-12-30 Y     #>  6 01-701-1047    85 >64    Placebo              2013-02-12 2013-03-09 Y     #>  7 01-701-1057    59 18-64  Screen Failure       NA         NA         NA    #>  8 01-701-1097    68 >64    Xanomeline Low Dose  2014-01-01 2014-07-09 Y     #>  9 01-701-1111    81 >64    Xanomeline Low Dose  2012-09-07 2012-09-16 Y     #> 10 01-701-1115    84 >64    Xanomeline Low Dose  2012-11-30 2013-01-23 Y"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"adae-adverse-events-analysis-dataset","dir":"Articles","previous_headings":"Building the Analytics Layer: ADaM Datasets (Gold Layer)","what":"ADAE: Adverse Events Analysis Dataset","title":"Clinical Trial Data Lake with ducklake","text":"ADAE provides analysis-ready adverse event data treatment-emergent flags severity grades.","code":"# Read ADSL for merging adsl <- get_ducklake_table(\"adsl\") |> collect() ae <- get_ducklake_table(\"ae\") |> collect()  # Build ADAE adae <- ae |>   # Merge ADSL variables   derive_vars_merged(     dataset_add = adsl,     new_vars = exprs(TRTSDT, TRTEDT, TRT01A, TRT01P),     by_vars = exprs(STUDYID, USUBJID)   ) |>   # Derive analysis dates   derive_vars_dt(     dtc = AESTDTC,     new_vars_prefix = \"AST\"   ) |>   derive_vars_dt(     dtc = AEENDTC,     new_vars_prefix = \"AEN\",     date_imputation = \"last\"   ) |>   # Derive treatment-emergent flag   mutate(     TRTEMFL = if_else(       !is.na(ASTDT) & !is.na(TRTSDT) & ASTDT >= TRTSDT,       \"Y\",       NA_character_     )   ) |>   # Derive analysis variables   mutate(     AOCCPFL = if_else(AESEQ == min(AESEQ), \"Y\", NA_character_),     AOCC01FL = AOCCPFL   ) |>   group_by(USUBJID, AEDECOD) |>   mutate(     AOCC01FL = if_else(row_number() == 1, \"Y\", NA_character_)   ) |>   ungroup()  # Store ADAE in the data lake with_transaction(   create_table(adae, \"adae\"),   author = \"T Gerke\",   commit_message = \"Create ADAE dataset\",   commit_extra_info = \"Includes treatment-emergent flags and occurrence flags\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Preview ADAE get_ducklake_table(\"adae\") |>   filter(TRTEMFL == \"Y\") |>   select(USUBJID, AEDECOD, ASTDT, AESEV, TRTEMFL) |>   head(10) #> # Source:   SQL [?? x 5] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>    USUBJID     AEDECOD                              ASTDT      AESEV    TRTEMFL #>    <chr>       <chr>                                <date>     <chr>    <chr>   #>  1 01-701-1015 APPLICATION SITE ERYTHEMA            2014-01-03 MILD     Y       #>  2 01-701-1015 APPLICATION SITE PRURITUS            2014-01-03 MILD     Y       #>  3 01-701-1015 DIARRHOEA                            2014-01-09 MILD     Y       #>  4 01-701-1023 ATRIOVENTRICULAR BLOCK SECOND DEGREE 2012-08-26 MILD     Y       #>  5 01-701-1023 ERYTHEMA                             2012-08-07 MILD     Y       #>  6 01-701-1023 ERYTHEMA                             2012-08-07 MODERATE Y       #>  7 01-701-1023 ERYTHEMA                             2012-08-07 MILD     Y       #>  8 01-701-1028 APPLICATION SITE ERYTHEMA            2013-07-21 MILD     Y       #>  9 01-701-1028 APPLICATION SITE PRURITUS            2013-08-08 MILD     Y       #> 10 01-701-1034 APPLICATION SITE PRURITUS            2014-08-27 MILD     Y"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"adpc-pharmacokinetic-concentrations-analysis-dataset","dir":"Articles","previous_headings":"Building the Analytics Layer: ADaM Datasets (Gold Layer)","what":"ADPC: Pharmacokinetic Concentrations Analysis Dataset","title":"Clinical Trial Data Lake with ducklake","text":"ADPC supports non-compartmental analysis combining PK concentrations dosing records.","code":"# Read required datasets adsl <- get_ducklake_table(\"adsl\") |> collect() pc <- get_ducklake_table(\"pc\") |> collect() ex <- get_ducklake_table(\"ex\") |> collect() vs <- get_ducklake_table(\"vs\") |> collect()  # Get ADSL variables needed adsl_vars <- exprs(TRTSDT, TRTSDTM, TRT01P, TRT01A)  # Derive PC dates and times pc_dates <- pc |>   derive_vars_merged(     dataset_add = adsl,     new_vars = adsl_vars,     by_vars = exprs(STUDYID, USUBJID)   ) |>   derive_vars_dtm(     new_vars_prefix = \"A\",     dtc = PCDTC,     time_imputation = \"00:00:00\",     ignore_seconds_flag = FALSE   ) |>   derive_vars_dtm_to_dt(exprs(ADTM)) |>   derive_vars_dtm_to_tm(exprs(ADTM)) |>   derive_vars_dy(reference_date = TRTSDT, source_vars = exprs(ADT)) |>   mutate(     EVID = 0,     NFRLT = if_else(PCTPTNUM < 0, 0, PCTPTNUM)   )  # Process exposure records ex_dates <- ex |>   derive_vars_merged(     dataset_add = adsl,     new_vars = adsl_vars,     by_vars = exprs(STUDYID, USUBJID)   ) |>   filter(EXDOSE > 0) |>   derive_vars_dtm(     new_vars_prefix = \"AST\",     dtc = EXSTDTC,     time_imputation = \"00:00:00\"   ) |>   mutate(     EVID = 1,     NFRLT = 24 * VISITDY   ) |>   derive_vars_dtm_to_dt(exprs(ASTDTM))  # Combine PC and EX adpc <- bind_rows(pc_dates, ex_dates) |>   arrange(STUDYID, USUBJID, ADTM) |>   mutate(     PARAMCD = coalesce(PCTESTCD, \"DOSE\"),     AVAL = case_when(       EVID == 1 ~ EXDOSE,       PCSTRESC == \"<BLQ\" & NFRLT == 0 ~ 0,       PCSTRESC == \"<BLQ\" & NFRLT > 0 ~ 0.5 * PCLLOQ,       TRUE ~ PCSTRESN     ),     PARAM = case_when(       PARAMCD == \"XAN\" ~ \"Xanomeline Concentration\",       PARAMCD == \"DOSE\" ~ \"Xanomeline Dose\"     )   )  # Store ADPC in the data lake with_transaction(   create_table(adpc, \"adpc\"),   author = \"T Gerke\",   commit_message = \"Create ADPC dataset\",   commit_extra_info = \"PK concentrations with dosing records for NCA\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Preview ADPC get_ducklake_table(\"adpc\") |>   filter(PARAMCD == \"XAN\") |>   select(USUBJID, ADT, PCTPT, AVAL, PARAM) |>   head(10) #> # Source:   SQL [?? x 5] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>    USUBJID     ADT        PCTPT             AVAL PARAM                    #>    <chr>       <date>     <chr>            <dbl> <chr>                    #>  1 01-701-1015 2014-01-01 Pre-dose         0     Xanomeline Concentration #>  2 01-701-1015 2014-01-02 5 Min Post-dose  0.005 Xanomeline Concentration #>  3 01-701-1015 2014-01-02 30 Min Post-dose 0.005 Xanomeline Concentration #>  4 01-701-1015 2014-01-02 1h Post-dose     0.005 Xanomeline Concentration #>  5 01-701-1015 2014-01-02 1.5h Post-dose   0.005 Xanomeline Concentration #>  6 01-701-1015 2014-01-02 2h Post-dose     0.005 Xanomeline Concentration #>  7 01-701-1015 2014-01-02 4h Post-dose     0.005 Xanomeline Concentration #>  8 01-701-1015 2014-01-02 6h Post-dose     0.005 Xanomeline Concentration #>  9 01-701-1015 2014-01-02 0-6h Post-dose   0.005 Xanomeline Concentration #> 10 01-701-1015 2014-01-02 8h Post-dose     0.005 Xanomeline Concentration"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"storing-regulatory-submission-artifacts","dir":"Articles","previous_headings":"","what":"Storing Regulatory Submission Artifacts","title":"Clinical Trial Data Lake with ducklake","text":"Beyond datasets, regulatory submissions require metadata documentation. data lake can store various types artifacts including define.xml files structured data:","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"define-xml-metadata","dir":"Articles","previous_headings":"Storing Regulatory Submission Artifacts","what":"Define.xml Metadata","title":"Clinical Trial Data Lake with ducklake","text":"define.xml file provides dataset variable-level metadata required regulatory submissions. Store versioned artifact:","code":"# Example: Store define.xml content # In practice, you might read this from a file generated by your metadata system define_xml <- '<?xml version=\"1.0\" encoding=\"UTF-8\"?> <ODM xmlns=\"http://www.cdisc.org/ns/odm/v1.3\">   <!-- Define.xml content for ADSL, ADAE, ADPC datasets --> <\/ODM>'  # Create a table for regulatory documents regulatory_docs <- tibble(   doc_type = \"define.xml\",   doc_version = \"1.0\",   content = define_xml,   created_date = Sys.Date(),   description = \"Dataset and variable metadata for regulatory submission\" )  with_transaction(   create_table(regulatory_docs, \"regulatory_documents\"),   author = \"T Gerke\",   commit_message = \"Add define.xml metadata\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  get_ducklake_table(\"regulatory_documents\") #> # Source:   table<regulatory_documents> [?? x 5] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>   doc_type   doc_version content                        created_date description #>   <chr>      <chr>       <chr>                          <date>       <chr>       #> 1 define.xml 1.0         \"<?xml version=\\\"1.0\\\" encodi… 2026-02-09   Dataset an…"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"storing-different-data-types-json-example","dir":"Articles","previous_headings":"Storing Regulatory Submission Artifacts","what":"Storing Different Data Types (JSON Example)","title":"Clinical Trial Data Lake with ducklake","text":"Analysis Results Metadata (ARM) Analysis Results Data (ARD) might stored separate data tables practice, example demonstrates store structured data JSON format within data lake:","code":"# Example: Store Analysis Results Metadata (ARM) arm_content <- tibble(   analysis_id = \"DEMO01\",   analysis_name = \"Demographics Table\",   dataset_used = \"ADSL\",   program_name = \"t_demographics.R\",   output_file = \"t_demographics.rtf\",   analysis_date = Sys.Date() )  # Add to or update regulatory documents table with_transaction(   get_ducklake_table(\"regulatory_documents\") |>     collect() |>     mutate(content = as.character(content)) |>     bind_rows(       tibble(         doc_type = \"ARM\",         doc_version = \"1.0\",         content = as.character(jsonlite::toJSON(arm_content)),         created_date = Sys.Date(),         description = \"Analysis Results Metadata\"       )     ) |>     distinct(doc_type, doc_version, .keep_all = TRUE) |>     replace_table(\"regulatory_documents\"),   author = \"T Gerke\",   commit_message = \"Add ARM metadata\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Example: Store Analysis Results Data (ARD) ard_content <- tibble(   analysis_id = \"DEMO01\",   row_type = \"header\",   row_label = \"Age (years)\",   treatment = c(\"Placebo\", \"Xanomeline Low\", \"Xanomeline High\"),   n = c(86, 84, 84),   mean = c(75.2, 75.7, 74.4),   sd = c(8.59, 7.89, 7.89) )  with_transaction(   get_ducklake_table(\"regulatory_documents\") |>     collect() |>     mutate(content = as.character(content)) |>     bind_rows(       tibble(         doc_type = \"ARD\",         doc_version = \"1.0\",         content = as.character(jsonlite::toJSON(ard_content)),         created_date = Sys.Date(),         description = \"Analysis Results Data for Demographics Table\"       )     ) |>     distinct(doc_type, doc_version, .keep_all = TRUE) |>     replace_table(\"regulatory_documents\"),   author = \"T Gerke\",   commit_message = \"Add demographics ARD\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"dataset-specifications","dir":"Articles","previous_headings":"Storing Regulatory Submission Artifacts","what":"Dataset Specifications","title":"Clinical Trial Data Lake with ducklake","text":"Store dataset specifications alongside data: unified approach ensures submission artifacts version-controlled alongside datasets describe, maintaining perfect alignment data documentation.","code":"# Example: Store ADSL specifications adsl_spec <- tibble(   dataset = \"ADSL\",   variable = c(\"USUBJID\", \"AGE\", \"AGEGR1\", \"TRT01P\", \"SAFFL\"),   label = c(     \"Unique Subject Identifier\",     \"Age\",     \"Age Group 1\",     \"Planned Treatment\",     \"Safety Population Flag\"   ),   type = c(\"text\", \"num\", \"text\", \"text\", \"text\"),   length = c(20, 8, 10, 40, 1),   derivation = c(\"DM.USUBJID\", \"DM.AGE\", \"Derived from AGE\", \"DM.ARM\", \"Derived\") )  with_transaction(   create_table(adsl_spec, \"dataset_specifications\"),   author = \"T Gerke\",   commit_message = \"Add ADSL specifications\" )  # Query specifications when needed get_ducklake_table(\"dataset_specifications\") |>   filter(dataset == \"ADSL\")"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"organizational-structure-and-cohesion","dir":"Articles","previous_headings":"","what":"Organizational Structure and Cohesion","title":"Clinical Trial Data Lake with ducklake","text":"One key advantages data lake approach preserves leverages inherently relational structure CDISC data.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"relational-structure-beyond-flat-files","dir":"Articles","previous_headings":"Organizational Structure and Cohesion","what":"Relational Structure: Beyond Flat Files","title":"Clinical Trial Data Lake with ducklake","text":"Many clinical trial workflows store SDTM domain ADaM dataset separate flat files (XPT, SAS7BDAT, CSV). meets regulatory requirements submission formats, often loses relational structure inherent CDISC standards day--day analysis work. Every SDTM domain shares STUDYID USUBJID keys, domains explicitly designed relate (e.g., EX records link DM subjects, AE records link DM EX). organizations use SAS datasets databases database solutions, DuckLake provides R-native approach preserves relationships version control built :","code":"# Traditional approach: Load multiple files, manually join # adsl <- read_xpt(\"adsl.xpt\") # ex <- read_xpt(\"ex.xpt\") # ae <- read_xpt(\"ae.xpt\") # result <- adsl |> left_join(ex, ...) |> left_join(ae, ...)  # DuckLake approach: Query across related tables directly using dplyr # Complex cross-domain query without loading all data adsl_tbl <- get_ducklake_table(\"adsl\") ex_tbl <- get_ducklake_table(\"ex\") ae_tbl <- get_ducklake_table(\"ae\")  adsl_tbl |>   filter(SAFFL == \"Y\") |>   left_join(ex_tbl, by = \"USUBJID\") |>   left_join(ae_tbl, by = \"USUBJID\") |>   group_by(USUBJID, AGE, TRT01P) |>   summarise(     n_aes = n_distinct(AESEQ, na.rm = TRUE),     total_dose = sum(EXDOSE, na.rm = TRUE),     .groups = \"drop\"   ) |>   arrange(desc(n_aes)) |>   head(10) |>   collect() #> # A tibble: 10 × 5 #>    USUBJID       AGE TRT01P               n_aes total_dose #>    <chr>       <dbl> <chr>                <dbl>      <dbl> #>  1 01-701-1302    61 Xanomeline High Dose    23       3105 #>  2 01-717-1004    80 Xanomeline Low Dose     19       3078 #>  3 01-718-1427    74 Xanomeline High Dose    16       2160 #>  4 01-704-1266    82 Xanomeline High Dose    16       2160 #>  5 01-709-1029    82 Xanomeline High Dose    16       3024 #>  6 01-701-1192    80 Xanomeline Low Dose     15       2430 #>  7 01-701-1275    61 Xanomeline High Dose    15       2025 #>  8 01-713-1179    64 Placebo                 15          0 #>  9 01-709-1309    65 Xanomeline High Dose    15       2835 #> 10 01-711-1143    76 Xanomeline Low Dose     14       1512"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"data-warehouse-benefits","dir":"Articles","previous_headings":"Organizational Structure and Cohesion","what":"Data Warehouse Benefits","title":"Clinical Trial Data Lake with ducklake","text":"approach provides traditional data warehouse capabilities clinical trials:","code":"# 1. Single source of truth - all datasets in one repository # List all tables in the data lake DBI::dbListTables(duckplyr:::get_default_duckdb_connection()) #>  [1] \"adae\"                                  #>  [2] \"adpc\"                                  #>  [3] \"adsl\"                                  #>  [4] \"ae\"                                    #>  [5] \"ae_raw\"                                #>  [6] \"dm\"                                    #>  [7] \"dm_raw\"                                #>  [8] \"ds\"                                    #>  [9] \"ds_raw\"                                #> [10] \"ducklake_column\"                       #> [11] \"ducklake_column_mapping\"               #> [12] \"ducklake_column_tag\"                   #> [13] \"ducklake_data_file\"                    #> [14] \"ducklake_delete_file\"                  #> [15] \"ducklake_file_column_stats\"            #> [16] \"ducklake_file_partition_value\"         #> [17] \"ducklake_files_scheduled_for_deletion\" #> [18] \"ducklake_inlined_data_tables\"          #> [19] \"ducklake_metadata\"                     #> [20] \"ducklake_name_mapping\"                 #> [21] \"ducklake_partition_column\"             #> [22] \"ducklake_partition_info\"               #> [23] \"ducklake_schema\"                       #> [24] \"ducklake_schema_versions\"              #> [25] \"ducklake_snapshot\"                     #> [26] \"ducklake_snapshot_changes\"             #> [27] \"ducklake_table\"                        #> [28] \"ducklake_table_column_stats\"           #> [29] \"ducklake_table_stats\"                  #> [30] \"ducklake_tag\"                          #> [31] \"ducklake_view\"                         #> [32] \"ex\"                                    #> [33] \"ex_raw\"                                #> [34] \"pc\"                                    #> [35] \"pc_raw\"                                #> [36] \"regulatory_documents\"                  #> [37] \"suppdm\"                                #> [38] \"suppdm_raw\"                            #> [39] \"vs\"                                    #> [40] \"vs_raw\"  # 2. Efficient filtering before loading into R # Only load subjects with adverse events get_ducklake_table(\"ae\") |>   filter(AESEV == \"SEVERE\") |>   distinct(USUBJID) #> # Source:   SQL [?? x 1] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>    USUBJID     #>    <chr>       #>  1 01-704-1008 #>  2 01-704-1445 #>  3 01-710-1070 #>  4 01-710-1368 #>  5 01-714-1195 #>  6 01-716-1189 #>  7 01-718-1427 #>  8 01-703-1086 #>  9 01-703-1119 #> 10 01-706-1049 #> # ℹ more rows  # 3. Aggregations performed at database level get_ducklake_table(\"adae\") |>   filter(TRTEMFL == \"Y\") |>   group_by(TRT01A, AESEV) |>   summarise(     n_events = n(),     n_subjects = n_distinct(USUBJID),     .groups = \"drop\"   ) #> # Source:   SQL [?? x 4] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>   TRT01A               AESEV    n_events n_subjects #>   <chr>                <chr>       <dbl>      <dbl> #> 1 Placebo              MODERATE       65         25 #> 2 Xanomeline Low Dose  SEVERE         25         16 #> 3 Placebo              MILD          210         58 #> 4 Xanomeline High Dose SEVERE         10          8 #> 5 Xanomeline High Dose MILD          287         65 #> 6 Xanomeline Low Dose  MODERATE      170         58 #> 7 Placebo              SEVERE          6          5 #> 8 Xanomeline Low Dose  MILD          232         64 #> 9 Xanomeline High Dose MODERATE      115         46  # 4. Joins across SDTM and ADaM layers # Example: Find date discrepancies between SDTM and ADaM ae_sdtm <- get_ducklake_table(\"ae\") |>   select(USUBJID, AESEQ, ae_date = AESTDTC, ae_term = AEDECOD)  adae_adam <- get_ducklake_table(\"adae\") |>   select(USUBJID, AESEQ, adae_date = ASTDT, adae_term = AEDECOD)  ae_sdtm |>   inner_join(adae_adam, by = c(\"USUBJID\", \"AESEQ\")) |>   # Convert SDTM character date to comparable format for filtering   mutate(ae_date_comparable = substr(ae_date, 1, 10)) |>   filter(ae_date_comparable != as.character(adae_date)) |>   select(     USUBJID,     sdtm_start_date = ae_date,     adam_start_date = adae_date,     sdtm_term = ae_term,     adam_term = adae_term   ) #> # Source:   SQL [?? x 5] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #> # ℹ 5 variables: USUBJID <chr>, sdtm_start_date <chr>, adam_start_date <date>, #> #   sdtm_term <chr>, adam_term <chr> # Note: This returns 0 rows with clean pharmaversesdtm data, # but demonstrates how to check for data quality issues across layers"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"cohesive-dataset-relationships","dir":"Articles","previous_headings":"Organizational Structure and Cohesion","what":"Cohesive Dataset Relationships","title":"Clinical Trial Data Lake with ducklake","text":"Let’s explore datasets connected: relational approach means clinical trial data lake functions purpose-built data warehouse, designed specifically relational nature CDISC standards.","code":"# List all tables in the data lake DBI::dbListTables(duckplyr:::get_default_duckdb_connection()) #>  [1] \"adae\"                                  #>  [2] \"adpc\"                                  #>  [3] \"adsl\"                                  #>  [4] \"ae\"                                    #>  [5] \"ae_raw\"                                #>  [6] \"dm\"                                    #>  [7] \"dm_raw\"                                #>  [8] \"ds\"                                    #>  [9] \"ds_raw\"                                #> [10] \"ducklake_column\"                       #> [11] \"ducklake_column_mapping\"               #> [12] \"ducklake_column_tag\"                   #> [13] \"ducklake_data_file\"                    #> [14] \"ducklake_delete_file\"                  #> [15] \"ducklake_file_column_stats\"            #> [16] \"ducklake_file_partition_value\"         #> [17] \"ducklake_files_scheduled_for_deletion\" #> [18] \"ducklake_inlined_data_tables\"          #> [19] \"ducklake_metadata\"                     #> [20] \"ducklake_name_mapping\"                 #> [21] \"ducklake_partition_column\"             #> [22] \"ducklake_partition_info\"               #> [23] \"ducklake_schema\"                       #> [24] \"ducklake_schema_versions\"              #> [25] \"ducklake_snapshot\"                     #> [26] \"ducklake_snapshot_changes\"             #> [27] \"ducklake_table\"                        #> [28] \"ducklake_table_column_stats\"           #> [29] \"ducklake_table_stats\"                  #> [30] \"ducklake_tag\"                          #> [31] \"ducklake_view\"                         #> [32] \"ex\"                                    #> [33] \"ex_raw\"                                #> [34] \"pc\"                                    #> [35] \"pc_raw\"                                #> [36] \"regulatory_documents\"                  #> [37] \"suppdm\"                                #> [38] \"suppdm_raw\"                            #> [39] \"vs\"                                    #> [40] \"vs_raw\"  # View snapshot history for key tables metadata_tables <- c(\"dm\", \"ex\", \"ae\", \"pc\",                       \"adsl\", \"adae\", \"adpc\")  # Collect snapshots for all tables purrr::map_dfr(metadata_tables, ~{   list_table_snapshots(.x) |>     mutate(table = .x, .before = 1) }) |>   select(table, snapshot_id, snapshot_time, changes) #>   table snapshot_id       snapshot_time #> 1    dm           2 2026-02-09 21:16:40 #> 2    ex           8 2026-02-09 21:16:41 #> 3    ae          10 2026-02-09 21:16:41 #> 4    pc          14 2026-02-09 21:16:41 #> 5  adsl          15 2026-02-09 21:16:42 #> 6  adae          16 2026-02-09 21:16:43 #> 7  adpc          17 2026-02-09 21:16:44 #>                                               changes #> 1    tables_created, tables_inserted_into, main.dm, 2 #> 2    tables_created, tables_inserted_into, main.ex, 8 #> 3   tables_created, tables_inserted_into, main.ae, 10 #> 4   tables_created, tables_inserted_into, main.pc, 14 #> 5 tables_created, tables_inserted_into, main.adsl, 15 #> 6 tables_created, tables_inserted_into, main.adae, 16 #> 7 tables_created, tables_inserted_into, main.adpc, 17  # Check all ADAE subjects exist in ADSL adae_tbl <- get_ducklake_table(\"adae\") adsl_tbl <- get_ducklake_table(\"adsl\")  integrity_check <- adae_tbl |>   anti_join(adsl_tbl, by = \"USUBJID\") |>   summarise(orphaned_records = n()) |>   collect()  # ADAE records without ADSL subject: integrity_check |> pull(orphaned_records) #> [1] 0"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"version-control-and-snapshots","dir":"Articles","previous_headings":"Demonstrating Core Functionality","what":"Version Control and Snapshots","title":"Clinical Trial Data Lake with ducklake","text":"Every modification tables automatically versioned. Let’s demonstrate adding new derived variables ADSL:","code":"# Add new derived columns using dplyr syntax # replace_table() handles the DROP/CREATE cycle internally with_transaction(   get_ducklake_table(\"adsl\") |>     mutate(       AGE65FL = if_else(AGE >= 65, \"Y\", \"N\"),       AGECAT = case_when(         AGE < 65 ~ \"<65\",         AGE >= 65 & AGE < 75 ~ \"65-74\",         AGE >= 75 ~ \">=75\",         TRUE ~ NA_character_       )     ) |>     replace_table(\"adsl\"),   author = \"T Gerke\",   commit_message = \"Add age categorization vars\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # View version history - should now show 2 snapshots list_table_snapshots(\"adsl\") #>    snapshot_id       snapshot_time schema_version #> 16          15 2026-02-09 21:16:42             15 #> 22          21 2026-02-09 21:16:45             21 #>                                                                    changes #> 16                     tables_created, tables_inserted_into, main.adsl, 15 #> 22 tables_created, tables_dropped, tables_inserted_into, main.adsl, 15, 21 #>     author              commit_message #> 16 T Gerke         Create ADSL dataset #> 22 T Gerke Add age categorization vars #>                                                                      commit_extra_info #> 16 Derived from DM, SUPPDM, DS, EX; includes treatment dates, safety flags, age groups #> 22                                                                                <NA>  # Verify new columns exist get_ducklake_table(\"adsl\") |>   select(USUBJID, AGE, AGE65FL, AGECAT) |>   head(5) #> # Source:   SQL [?? x 4] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>   USUBJID       AGE AGE65FL AGECAT #>   <chr>       <dbl> <chr>   <chr>  #> 1 01-701-1015    63 N       <65    #> 2 01-701-1023    64 N       <65    #> 3 01-701-1028    71 Y       65-74  #> 4 01-701-1033    74 Y       65-74  #> 5 01-701-1034    77 Y       >=75"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"best-practice-use-replace_table-for-all-modifications","dir":"Articles","previous_headings":"Demonstrating Core Functionality > Version Control and Snapshots","what":"Best Practice: Use replace_table() for All Modifications","title":"Clinical Trial Data Lake with ducklake","text":"clinical trial data workflows, always use replace_table() wrapped with_transaction() ensure complete audit trails regulatory compliance. replace_table() essential GxP work: ✅ Creates versioned snapshots - Every change recorded can time-traveled back ✅ Complete audit trails - Records changed, , ✅ Regulatory compliance - Meets 21 CFR Part 11 ICH GCP requirements ✅ Data lineage - Proves data integrity regulatory inspections ✅ Schema flexibility - Add/remove columns modify values approach ✅ Natural dplyr interface - get_ducklake_table(name) |> mutate(...) |> replace_table(name) modifications clinical trials, use pattern: operations create snapshots can time-travel back include regulatory audit trail.","code":"# Correcting a specific value - creates auditable snapshot with_transaction(   get_ducklake_table(\"adsl\") |>     mutate(SAFFL = if_else(USUBJID == \"01-701-1015\", \"N\", SAFFL)) |>     replace_table(\"adsl\"),   author = \"T Gerke\",   commit_message = \"Correct safety flag\" )  # Adding new derived columns - creates auditable snapshot with_transaction(   get_ducklake_table(\"adsl\") |>     mutate(AGE65FL = if_else(AGE >= 65, \"Y\", \"N\")) |>     replace_table(\"adsl\"),   author = \"T Gerke\",   commit_message = \"Add age 65+ flag\" )"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"iterative-development-with-full-audit-trail","dir":"Articles","previous_headings":"Demonstrating Core Functionality > Version Control and Snapshots","what":"Iterative Development with Full Audit Trail","title":"Clinical Trial Data Lake with ducklake","text":"developing derivations, create snapshot meaningful iteration maintain complete audit trail: GxP-compliant approach ensures: Complete audit trail derivation iterations Ability recreate intermediate state Proof changed regulatory inspections Data lineage initial final derivation","code":"# Iteration 1: First attempt (creates snapshot v2) with_transaction(   get_ducklake_table(\"adsl\") |>     mutate(AGECAT_TEST = case_when(       AGE < 50 ~ \"Young\",       AGE >= 50 ~ \"Older\"     )) |>     replace_table(\"adsl\"),   author = \"T Gerke\",   commit_message = \"Test age categories v1\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Iteration 2: Refinement (creates snapshot v3) with_transaction(   get_ducklake_table(\"adsl\") |>     mutate(AGECAT_TEST = case_when(       AGE < 40 ~ \"18-39\",       AGE < 65 ~ \"40-64\",       AGE >= 65 ~ \"65+\"     )) |>     replace_table(\"adsl\"),   author = \"T Gerke\",   commit_message = \"Refine age categories v2\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Iteration 3: Final version (creates snapshot v4) with_transaction(   get_ducklake_table(\"adsl\") |>     mutate(       AGECAT_TEST = NULL,       AGECAT2 = case_when(         AGE < 40 ~ \"18-39\",         AGE < 65 ~ \"40-64\",         AGE >= 65 ~ \"65+\",         TRUE ~ \"Missing\"       )     ) |>     replace_table(\"adsl\"),   author = \"T Gerke\",   commit_message = \"Finalize age categories\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Complete audit trail available snapshots <- list_table_snapshots(\"adsl\") snapshots  # Shows all iterations with snapshot metadata #>    snapshot_id       snapshot_time schema_version #> 16          15 2026-02-09 21:16:42             15 #> 22          21 2026-02-09 21:16:45             21 #> 23          22 2026-02-09 21:16:45             22 #> 24          23 2026-02-09 21:16:45             23 #> 25          24 2026-02-09 21:16:46             24 #>                                                                    changes #> 16                     tables_created, tables_inserted_into, main.adsl, 15 #> 22 tables_created, tables_dropped, tables_inserted_into, main.adsl, 15, 21 #> 23 tables_created, tables_dropped, tables_inserted_into, main.adsl, 21, 22 #> 24 tables_created, tables_dropped, tables_inserted_into, main.adsl, 22, 23 #> 25 tables_created, tables_dropped, tables_inserted_into, main.adsl, 23, 24 #>     author              commit_message #> 16 T Gerke         Create ADSL dataset #> 22 T Gerke Add age categorization vars #> 23 T Gerke      Test age categories v1 #> 24 T Gerke    Refine age categories v2 #> 25 T Gerke     Finalize age categories #>                                                                      commit_extra_info #> 16 Derived from DM, SUPPDM, DS, EX; includes treatment dates, safety flags, age groups #> 22                                                                                <NA> #> 23                                                                                <NA> #> 24                                                                                <NA> #> 25                                                                                <NA>  # Each row represents a point in time you can restore to # - snapshot_id: Unique identifier for this version # - snapshot_time: When this version was created # - changes: What operations created this snapshot  # Time-travel to specific snapshots using snapshot_id # Use actual snapshot IDs from the list (first, second, and last) snapshot_ids <- snapshots$snapshot_id adsl_v1 <- get_ducklake_table_version(\"adsl\", snapshot_ids[1]) adsl_v2 <- get_ducklake_table_version(\"adsl\", snapshot_ids[2]) adsl_final <- get_ducklake_table_version(\"adsl\", snapshot_ids[length(snapshot_ids)])  # Or use snapshot times for time-travel # Note: Add 1 second to ensure we query AFTER the snapshot was created adsl_asof <- get_ducklake_table_asof(\"adsl\", snapshots$snapshot_time[2] + 1)  # Compare columns across iterations colnames(adsl_v1 |> collect())  # Initial version #>  [1] \"STUDYID\"  \"DOMAIN\"   \"USUBJID\"  \"SUBJID\"   \"RFSTDTC\"  \"RFENDTC\"  #>  [7] \"RFXSTDTC\" \"RFXENDTC\" \"RFICDTC\"  \"RFPENDTC\" \"DTHDTC\"   \"DTHFL\"    #> [13] \"SITEID\"   \"BRTHDTC\"  \"AGE\"      \"AGEU\"     \"SEX\"      \"RACE\"     #> [19] \"ETHNIC\"   \"ARMCD\"    \"ARM\"      \"ACTARMCD\" \"ACTARM\"   \"COUNTRY\"  #> [25] \"DMDTC\"    \"DMDY\"     \"TRTSDTM\"  \"TRTEDTM\"  \"TRTSDT\"   \"TRTEDT\"   #> [31] \"TRTDURD\"  \"SAFFL\"    \"TRT01P\"   \"TRT01A\"   \"AGEGR1\"   \"AGEGR1N\"  #> [37] \"RANDDT\"   \"EOSDT\"    \"EOSSTT\" colnames(adsl_final |> collect())  # Final version with all derivations #>  [1] \"STUDYID\"  \"DOMAIN\"   \"USUBJID\"  \"SUBJID\"   \"RFSTDTC\"  \"RFENDTC\"  #>  [7] \"RFXSTDTC\" \"RFXENDTC\" \"RFICDTC\"  \"RFPENDTC\" \"DTHDTC\"   \"DTHFL\"    #> [13] \"SITEID\"   \"BRTHDTC\"  \"AGE\"      \"AGEU\"     \"SEX\"      \"RACE\"     #> [19] \"ETHNIC\"   \"ARMCD\"    \"ARM\"      \"ACTARMCD\" \"ACTARM\"   \"COUNTRY\"  #> [25] \"DMDTC\"    \"DMDY\"     \"TRTSDTM\"  \"TRTEDTM\"  \"TRTSDT\"   \"TRTEDT\"   #> [31] \"TRTDURD\"  \"SAFFL\"    \"TRT01P\"   \"TRT01A\"   \"AGEGR1\"   \"AGEGR1N\"  #> [37] \"RANDDT\"   \"EOSDT\"    \"EOSSTT\"   \"AGE65FL\"  \"AGECAT\"   \"AGECAT2\""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"time-travel","dir":"Articles","previous_headings":"Demonstrating Core Functionality","what":"Time Travel","title":"Clinical Trial Data Lake with ducklake","text":"Query data existed specific point time:","code":"# Get the current version adsl_current <- get_ducklake_table(\"adsl\")  # Get the version history for adsl versions <- list_table_snapshots(\"adsl\") print(versions) #>    snapshot_id       snapshot_time schema_version #> 16          15 2026-02-09 21:16:42             15 #> 22          21 2026-02-09 21:16:45             21 #> 23          22 2026-02-09 21:16:45             22 #> 24          23 2026-02-09 21:16:45             23 #> 25          24 2026-02-09 21:16:46             24 #>                                                                    changes #> 16                     tables_created, tables_inserted_into, main.adsl, 15 #> 22 tables_created, tables_dropped, tables_inserted_into, main.adsl, 15, 21 #> 23 tables_created, tables_dropped, tables_inserted_into, main.adsl, 21, 22 #> 24 tables_created, tables_dropped, tables_inserted_into, main.adsl, 22, 23 #> 25 tables_created, tables_dropped, tables_inserted_into, main.adsl, 23, 24 #>     author              commit_message #> 16 T Gerke         Create ADSL dataset #> 22 T Gerke Add age categorization vars #> 23 T Gerke      Test age categories v1 #> 24 T Gerke    Refine age categories v2 #> 25 T Gerke     Finalize age categories #>                                                                      commit_extra_info #> 16 Derived from DM, SUPPDM, DS, EX; includes treatment dates, safety flags, age groups #> 22                                                                                <NA> #> 23                                                                                <NA> #> 24                                                                                <NA> #> 25                                                                                <NA>  # Get data from the first snapshot version first_snapshot_id <- versions |>   slice(1) |>   pull(snapshot_id)  adsl_v1 <- get_ducklake_table_version(   table_name = \"adsl\",   version = first_snapshot_id )  # Compare versions - earlier version shouldn't have derived variables added later adsl_v1 |> collect() #> # A tibble: 306 × 39 #>    STUDYID      DOMAIN USUBJID  SUBJID RFSTDTC RFENDTC RFXSTDTC RFXENDTC RFICDTC #>    <chr>        <chr>  <chr>    <chr>  <chr>   <chr>   <chr>    <chr>    <chr>   #>  1 CDISCPILOT01 DM     01-701-… 1015   2014-0… 2014-0… 2014-01… 2014-07… NA      #>  2 CDISCPILOT01 DM     01-701-… 1023   2012-0… 2012-0… 2012-08… 2012-09… NA      #>  3 CDISCPILOT01 DM     01-701-… 1028   2013-0… 2014-0… 2013-07… 2014-01… NA      #>  4 CDISCPILOT01 DM     01-701-… 1033   2014-0… 2014-0… 2014-03… 2014-03… NA      #>  5 CDISCPILOT01 DM     01-701-… 1034   2014-0… 2014-1… 2014-07… 2014-12… NA      #>  6 CDISCPILOT01 DM     01-701-… 1047   2013-0… 2013-0… 2013-02… 2013-03… NA      #>  7 CDISCPILOT01 DM     01-701-… 1057   NA      NA      NA       NA       NA      #>  8 CDISCPILOT01 DM     01-701-… 1097   2014-0… 2014-0… 2014-01… 2014-07… NA      #>  9 CDISCPILOT01 DM     01-701-… 1111   2012-0… 2012-0… 2012-09… 2012-09… NA      #> 10 CDISCPILOT01 DM     01-701-… 1115   2012-1… 2013-0… 2012-11… 2013-01… NA      #> # ℹ 296 more rows #> # ℹ 30 more variables: RFPENDTC <chr>, DTHDTC <chr>, DTHFL <chr>, SITEID <chr>, #> #   BRTHDTC <chr>, AGE <dbl>, AGEU <chr>, SEX <chr>, RACE <chr>, ETHNIC <chr>, #> #   ARMCD <chr>, ARM <chr>, ACTARMCD <chr>, ACTARM <chr>, COUNTRY <chr>, #> #   DMDTC <chr>, DMDY <dbl>, TRTSDTM <dttm>, TRTEDTM <dttm>, TRTSDT <date>, #> #   TRTEDT <date>, TRTDURD <dbl>, SAFFL <chr>, TRT01P <chr>, TRT01A <chr>, #> #   AGEGR1 <chr>, AGEGR1N <dbl>, RANDDT <date>, EOSDT <date>, EOSSTT <chr> adsl_current |> collect() #> # A tibble: 306 × 42 #>    STUDYID      DOMAIN USUBJID  SUBJID RFSTDTC RFENDTC RFXSTDTC RFXENDTC RFICDTC #>    <chr>        <chr>  <chr>    <chr>  <chr>   <chr>   <chr>    <chr>    <chr>   #>  1 CDISCPILOT01 DM     01-701-… 1015   2014-0… 2014-0… 2014-01… 2014-07… NA      #>  2 CDISCPILOT01 DM     01-701-… 1023   2012-0… 2012-0… 2012-08… 2012-09… NA      #>  3 CDISCPILOT01 DM     01-701-… 1028   2013-0… 2014-0… 2013-07… 2014-01… NA      #>  4 CDISCPILOT01 DM     01-701-… 1033   2014-0… 2014-0… 2014-03… 2014-03… NA      #>  5 CDISCPILOT01 DM     01-701-… 1034   2014-0… 2014-1… 2014-07… 2014-12… NA      #>  6 CDISCPILOT01 DM     01-701-… 1047   2013-0… 2013-0… 2013-02… 2013-03… NA      #>  7 CDISCPILOT01 DM     01-701-… 1057   NA      NA      NA       NA       NA      #>  8 CDISCPILOT01 DM     01-701-… 1097   2014-0… 2014-0… 2014-01… 2014-07… NA      #>  9 CDISCPILOT01 DM     01-701-… 1111   2012-0… 2012-0… 2012-09… 2012-09… NA      #> 10 CDISCPILOT01 DM     01-701-… 1115   2012-1… 2013-0… 2012-11… 2013-01… NA      #> # ℹ 296 more rows #> # ℹ 33 more variables: RFPENDTC <chr>, DTHDTC <chr>, DTHFL <chr>, SITEID <chr>, #> #   BRTHDTC <chr>, AGE <dbl>, AGEU <chr>, SEX <chr>, RACE <chr>, ETHNIC <chr>, #> #   ARMCD <chr>, ARM <chr>, ACTARMCD <chr>, ACTARM <chr>, COUNTRY <chr>, #> #   DMDTC <chr>, DMDY <dbl>, TRTSDTM <dttm>, TRTEDTM <dttm>, TRTSDT <date>, #> #   TRTEDT <date>, TRTDURD <dbl>, SAFFL <chr>, TRT01P <chr>, TRT01A <chr>, #> #   AGEGR1 <chr>, AGEGR1N <dbl>, RANDDT <date>, EOSDT <date>, EOSSTT <chr>, …"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"transactions-for-atomic-updates","dir":"Articles","previous_headings":"Demonstrating Core Functionality","what":"Transactions for Atomic Updates","title":"Clinical Trial Data Lake with ducklake","text":"Transactions ensure related table updates either succeed fail together, maintaining data consistency. critical adding derived variables must stay synchronized across datasets. ’s example adding new analysis flag ADSL ADAE atomically: ensures ADSL ADAE stay synchronized - either get new ANALYSISFL column neither . with_transaction() function automatically handles rollback operation fails, making safer manually managing transactions. updates also versioned audit trails.","code":"# Add ANALYSISFL to both ADSL and ADAE in a single atomic operation # with_transaction() automatically handles rollback on error with_transaction({   # First, add the flag to ADSL   get_ducklake_table(\"adsl\") |>     mutate(ANALYSISFL = if_else(SAFFL == \"Y\" & !is.na(TRTSDT), \"Y\", \"N\")) |>     replace_table(\"adsl\")  # Creates versioned snapshot      # Then propagate to ADAE by joining   adsl_flags <- get_ducklake_table(\"adsl\") |>     select(USUBJID, ANALYSISFL)      get_ducklake_table(\"adae\") |>     select(-any_of(\"ANALYSISFL\")) |>  # Remove if exists     left_join(adsl_flags, by = \"USUBJID\") |>     replace_table(\"adae\")  # Creates versioned snapshot      # Both updates succeed together   cat(\"Both tables updated successfully\\n\") }, author = \"T Gerke\", commit_message = \"Add analysis flag\") #> Transaction started #> Both tables updated successfully #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"updating-records","dir":"Articles","previous_headings":"Demonstrating Core Functionality","what":"Updating Records","title":"Clinical Trial Data Lake with ducklake","text":"Update existing records maintaining version control audit trails:","code":"# Update a specific record with versioning with_transaction(   get_ducklake_table(\"adae\") |>     mutate(       AESEV = if_else(         USUBJID == \"01-701-1015\" & AESEQ == 1,         \"SEVERE\",         AESEV       )     ) |>     replace_table(\"adae\"),  # Creates versioned snapshot   author = \"T Gerke\",   commit_message = \"Correct AE severity\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Verify the update get_ducklake_table(\"adae\") |>   filter(USUBJID == \"01-701-1015\", AESEQ == 1) |>   select(USUBJID, AEDECOD, AESEV) #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #>   USUBJID     AEDECOD                   AESEV  #>   <chr>       <chr>                     <chr>  #> 1 01-701-1015 APPLICATION SITE ERYTHEMA SEVERE"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"querying-and-analysis","dir":"Articles","previous_headings":"","what":"Querying and Analysis","title":"Clinical Trial Data Lake with ducklake","text":"data lake enables efficient querying across datasets:","code":"# Example 1: Subject disposition summary get_ducklake_table(\"adsl\") |>   count(EOSSTT, TRT01P) |>   arrange(TRT01P, EOSSTT) #> # Source:     SQL [?? x 3] #> # Database:   DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #> # Ordered by: TRT01P, EOSSTT #>   EOSSTT    TRT01P                   n #>   <chr>     <chr>                <dbl> #> 1 COMPLETED Placebo                 86 #> 2 ONGOING   Screen Failure          52 #> 3 COMPLETED Xanomeline High Dose    84 #> 4 COMPLETED Xanomeline Low Dose     84  # Example 2: Treatment-emergent AE summary by severity get_ducklake_table(\"adae\") |>   filter(TRTEMFL == \"Y\") |>   count(TRT01A, AESEV) |>   arrange(TRT01A, AESEV) #> # Source:     SQL [?? x 3] #> # Database:   DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #> # Ordered by: TRT01A, AESEV #>   TRT01A               AESEV        n #>   <chr>                <chr>    <dbl> #> 1 Placebo              MILD       209 #> 2 Placebo              MODERATE    65 #> 3 Placebo              SEVERE       7 #> 4 Xanomeline High Dose MILD       287 #> 5 Xanomeline High Dose MODERATE   115 #> 6 Xanomeline High Dose SEVERE      10 #> 7 Xanomeline Low Dose  MILD       232 #> 8 Xanomeline Low Dose  MODERATE   170 #> 9 Xanomeline Low Dose  SEVERE      25  # Example 3: PK concentration profile get_ducklake_table(\"adpc\") |>   filter(PARAMCD == \"XAN\", EVID == 0) |>   group_by(NFRLT) |>   summarise(     n = n(),     mean_conc = mean(AVAL, na.rm = TRUE),     sd_conc = sd(AVAL, na.rm = TRUE)   ) |>   arrange(NFRLT) #> # Source:     SQL [?? x 4] #> # Database:   DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #> # Ordered by: NFRLT #>    NFRLT     n mean_conc  sd_conc #>    <dbl> <dbl>     <dbl>    <dbl> #>  1  0      254   0        0       #>  2  0.08   254   0.0682   0.0455  #>  3  0.5    254   0.362    0.257   #>  4  1      254   0.616    0.439   #>  5  1.5    254   0.795    0.568   #>  6  2      254   0.922    0.658   #>  7  3      254  17.7     12.7     #>  8  4      254   1.15     0.821   #>  9  6      254   1.21     0.862   #> 10  8      254   1.22     0.872   #> 11  9      254  14.9     10.8     #> 12 12      254   0.366    0.260   #> 13 16      254   0.110    0.0767  #> 14 18      254   9.39     6.92    #> 15 24      254   0.0114   0.00520 #> 16 36      254   0.00500  0       #> 17 37      254   0.165    0.426   #> 18 48      254   0.00500  0  # Example 4: Cross-domain analysis: AEs by age group get_ducklake_table(\"adae\") |>   filter(TRTEMFL == \"Y\") |>   left_join(     get_ducklake_table(\"adsl\") |>       select(USUBJID, AGEGR1, TRT01A),     by = \"USUBJID\"   ) |>   count(AGEGR1, TRT01A.x) |>   arrange(AGEGR1, TRT01A.x) #> # Source:     SQL [?? x 3] #> # Database:   DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpXQSKxf/duckplyr/duckplyr1f177927e1db.duckdb] #> # Ordered by: AGEGR1, TRT01A.x #>   AGEGR1 TRT01A.x                 n #>   <chr>  <chr>                <dbl> #> 1 18-64  Placebo                 57 #> 2 18-64  Xanomeline High Dose    86 #> 3 18-64  Xanomeline Low Dose     20 #> 4 >64    Placebo                224 #> 5 >64    Xanomeline High Dose   326 #> 6 >64    Xanomeline Low Dose    407"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"audit-trail-and-compliance","dir":"Articles","previous_headings":"","what":"Audit Trail and Compliance","title":"Clinical Trial Data Lake with ducklake","text":"regulatory submissions, complete audit trail essential:","code":"# Generate audit report for ADSL audit_report <- list_table_snapshots(\"adsl\") audit_report #>    snapshot_id       snapshot_time schema_version #> 16          15 2026-02-09 21:16:42             15 #> 22          21 2026-02-09 21:16:45             21 #> 23          22 2026-02-09 21:16:45             22 #> 24          23 2026-02-09 21:16:45             23 #> 25          24 2026-02-09 21:16:46             24 #> 26          25 2026-02-09 21:16:46             25 #>                                                                                       changes #> 16                                        tables_created, tables_inserted_into, main.adsl, 15 #> 22                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 15, 21 #> 23                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 21, 22 #> 24                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 22, 23 #> 25                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 23, 24 #> 26 tables_created, tables_dropped, tables_inserted_into, main.adsl, main.adae, 16, 24, 25, 26 #>     author              commit_message #> 16 T Gerke         Create ADSL dataset #> 22 T Gerke Add age categorization vars #> 23 T Gerke      Test age categories v1 #> 24 T Gerke    Refine age categories v2 #> 25 T Gerke     Finalize age categories #> 26 T Gerke           Add analysis flag #>                                                                      commit_extra_info #> 16 Derived from DM, SUPPDM, DS, EX; includes treatment dates, safety flags, age groups #> 22                                                                                <NA> #> 23                                                                                <NA> #> 24                                                                                <NA> #> 25                                                                                <NA> #> 26                                                                                <NA>  # Get table metadata from DuckLake system tables adsl_table_meta <- get_metadata_table(\"ducklake_table\") |>   filter(table_name == \"adsl\") |>   collect() adsl_table_meta #> # A tibble: 6 × 8 #>   table_id table_uuid     begin_snapshot end_snapshot schema_id table_name path  #>      <dbl> <chr>                   <dbl>        <dbl>     <dbl> <chr>      <chr> #> 1       15 019c4443-8a3f…             15           21         0 adsl       adsl/ #> 2       21 019c4443-9524…             21           22         0 adsl       adsl/ #> 3       22 019c4443-95f5…             22           23         0 adsl       adsl/ #> 4       23 019c4443-96a7…             23           24         0 adsl       adsl/ #> 5       24 019c4443-975a…             24           25         0 adsl       adsl/ #> 6       26 019c4443-9904…             25           NA         0 adsl       adsl/ #> # ℹ 1 more variable: path_is_relative <lgl>  # Export audit information audit_export <- audit_report |>   mutate(     table_name = \"adsl\",     dataset_label = \"Subject-Level Analysis Dataset\"   ) audit_export #>    snapshot_id       snapshot_time schema_version #> 16          15 2026-02-09 21:16:42             15 #> 22          21 2026-02-09 21:16:45             21 #> 23          22 2026-02-09 21:16:45             22 #> 24          23 2026-02-09 21:16:45             23 #> 25          24 2026-02-09 21:16:46             24 #> 26          25 2026-02-09 21:16:46             25 #>                                                                                       changes #> 16                                        tables_created, tables_inserted_into, main.adsl, 15 #> 22                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 15, 21 #> 23                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 21, 22 #> 24                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 22, 23 #> 25                    tables_created, tables_dropped, tables_inserted_into, main.adsl, 23, 24 #> 26 tables_created, tables_dropped, tables_inserted_into, main.adsl, main.adae, 16, 24, 25, 26 #>     author              commit_message #> 16 T Gerke         Create ADSL dataset #> 22 T Gerke Add age categorization vars #> 23 T Gerke      Test age categories v1 #> 24 T Gerke    Refine age categories v2 #> 25 T Gerke     Finalize age categories #> 26 T Gerke           Add analysis flag #>                                                                      commit_extra_info #> 16 Derived from DM, SUPPDM, DS, EX; includes treatment dates, safety flags, age groups #> 22                                                                                <NA> #> 23                                                                                <NA> #> 24                                                                                <NA> #> 25                                                                                <NA> #> 26                                                                                <NA> #>    table_name                  dataset_label #> 16       adsl Subject-Level Analysis Dataset #> 22       adsl Subject-Level Analysis Dataset #> 23       adsl Subject-Level Analysis Dataset #> 24       adsl Subject-Level Analysis Dataset #> 25       adsl Subject-Level Analysis Dataset #> 26       adsl Subject-Level Analysis Dataset"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"cleanup","dir":"Articles","previous_headings":"","what":"Cleanup","title":"Clinical Trial Data Lake with ducklake","text":"’re done, can detach data lake:","code":"detach_ducklake()"},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Clinical Trial Data Lake with ducklake","text":"vignette demonstrated ducklake provides robust infrastructure clinical trial data management: Setup: Created versioned data lake clinical trial data Medallion Architecture: Implemented bronze (raw), silver (cleaned), gold (analysis) layers SDTM Loading: Loaded multiple SDTM domains raw cleaned versions full version control ADaM Derivation: Built analysis datasets (ADSL, ADAE, ADPC) complete data lineage silver gold Regulatory Artifacts: Stored define.xml, ARD, ARM, specifications alongside datasets Organization: Maintained cohesive relationships related datasets documentation Functionality: Demonstrated versioning, time travel, transactions, record updates Analysis: Showed efficient cross-domain queries Compliance: Generated audit trails regulatory requirements raw data preservation using ducklake clinical trial data, ensure: Modern Architecture: Relational database structure inherently relational CDISC data Layered Design: Bronze/silver/gold layers separate raw, cleaned, analysis-ready data Reproducibility: Analyses can exactly recreated; raw data enables reprocessing Traceability: Complete lineage raw source cleaning final analysis Collaboration: Multiple analysts working safely shared data layers Compliance: Regulatory-ready audit trails preserved source data Efficiency: Fast queries across related datasets without loading multiple flat files Data Integrity: Referential integrity checks across related tables Reprocessability: Ability rerun cleaning analysis logic without re-extracting EDC information specific features: vignette(\"ducklake\") - Getting started guide vignette(\"time-travel\") - Time travel version control vignette(\"transactions\") - Transaction management","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/clinical-trial-datalake.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Clinical Trial Data Lake with ducklake","text":"CDISC SDTM CDISC ADaM pharmaverse admiral pharmaversesdtm DuckLake Documentation","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"ducklake Cookbook","text":"cookbook provides quick recipes common ducklake operations. recipe self-contained example can adapt workflow. comprehensive real-world example, see clinical trial data lake vignette.","code":""},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"create-a-new-data-lake","dir":"Articles","previous_headings":"Setup recipes","what":"Create a new data lake","title":"ducklake Cookbook","text":"","code":"# Create a data lake in a specific directory attach_ducklake(\"my_lake\", lake_path = vignette_temp_dir)"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"attach-to-an-existing-data-lake","dir":"Articles","previous_headings":"Setup recipes","what":"Attach to an existing data lake","title":"ducklake Cookbook","text":"","code":"# Attach to an existing lake (creates it if it doesn't exist) attach_ducklake(\"existing_lake\", lake_path = \"/path/to/data_lake\")"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"detach-from-a-data-lake","dir":"Articles","previous_headings":"Setup recipes","what":"Detach from a data lake","title":"ducklake Cookbook","text":"","code":"# Detach when done (doesn't delete the lake) detach_ducklake(\"my_lake\")"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"load-data-from-a-data-frame","dir":"Articles","previous_headings":"Loading data recipes","what":"Load data from a data.frame","title":"ducklake Cookbook","text":"","code":"with_transaction(   create_table(mtcars, \"cars\"),   author = \"Data Engineer\",   commit_message = \"Initial car data load\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"update-an-existing-table","dir":"Articles","previous_headings":"Loading data recipes","what":"Update an existing table","title":"ducklake Cookbook","text":"","code":"# Create a second version of the cars table with_transaction(   get_ducklake_table(\"cars\") |>     mutate(kpl = mpg * 0.425144) |>  # Add km/L conversion     replace_table(\"cars\"),   author = \"Data Engineer\",   commit_message = \"Add km/L metric to cars table\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"load-data-from-a-csv-file","dir":"Articles","previous_headings":"Loading data recipes","what":"Load data from a CSV file","title":"ducklake Cookbook","text":"","code":"# First write a sample CSV (in practice, you'd have an existing file) csv_path <- file.path(vignette_temp_dir, \"sample_data.csv\") write.csv(head(iris, 20), csv_path, row.names = FALSE)  # Load the CSV into the data lake with_transaction(   create_table(csv_path, \"iris_sample\"),   author = \"Data Engineer\",   commit_message = \"Load iris sample from CSV\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"load-data-from-a-url","dir":"Articles","previous_headings":"Loading data recipes","what":"Load data from a URL","title":"ducklake Cookbook","text":"","code":"# ducklake can load data directly from URLs with_transaction(   create_table(\"https://example.com/data.csv\", \"remote_data\"),   author = \"Data Engineer\",   commit_message = \"Load remote dataset\" )"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"load-with-a-dplyr-pipeline","dir":"Articles","previous_headings":"Loading data recipes","what":"Load with a dplyr pipeline","title":"ducklake Cookbook","text":"","code":"with_transaction(   mtcars |>     filter(mpg > 20) |>     create_table(\"efficient_cars\"),   author = \"Data Analyst\",   commit_message = \"Load filtered car data\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"list-all-tables-in-the-lake","dir":"Articles","previous_headings":"Loading data recipes","what":"List all tables in the lake","title":"ducklake Cookbook","text":"","code":"# See what tables exist in your lake get_ducklake_table(\"duckdb_tables\") |>   filter(schema_name == \"main\") |>   select(table_name) |>   collect() |>   print(n = Inf) #> # A tibble: 25 × 1 #>    table_name                            #>    <chr>                                 #>  1 ducklake_column                       #>  2 ducklake_column_mapping               #>  3 ducklake_column_tag                   #>  4 ducklake_data_file                    #>  5 ducklake_delete_file                  #>  6 ducklake_files_scheduled_for_deletion #>  7 ducklake_file_column_stats            #>  8 ducklake_file_partition_value         #>  9 ducklake_inlined_data_tables          #> 10 ducklake_metadata                     #> 11 ducklake_name_mapping                 #> 12 ducklake_partition_column             #> 13 ducklake_partition_info               #> 14 ducklake_schema                       #> 15 ducklake_schema_versions              #> 16 ducklake_snapshot                     #> 17 ducklake_snapshot_changes             #> 18 ducklake_table                        #> 19 ducklake_table_column_stats           #> 20 ducklake_table_stats                  #> 21 ducklake_tag                          #> 22 ducklake_view                         #> 23 efficient_cars                        #> 24 iris_sample                           #> 25 cars"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"read-a-table","dir":"Articles","previous_headings":"Reading data recipes","what":"Read a table","title":"ducklake Cookbook","text":"","code":"# Returns a lazy dplyr tbl cars_data <- get_ducklake_table(\"cars\")  # Use dplyr verbs cars_data |>   filter(cyl == 6) |>   select(mpg, cyl, hp) |>   head(3) #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/Rtmps7yUcv/duckplyr/duckplyr1f94478baa40.duckdb] #>     mpg   cyl    hp #>   <dbl> <dbl> <dbl> #> 1  21       6   110 #> 2  21       6   110 #> 3  21.4     6   110"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"collect-data-into-memory","dir":"Articles","previous_headings":"Reading data recipes","what":"Collect data into memory","title":"ducklake Cookbook","text":"","code":"# Fetch all data into a data.frame cars_df <- get_ducklake_table(\"cars\") |> collect() head(cars_df, 3) #> # A tibble: 3 × 12 #>     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb   kpl #>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4  8.93 #> 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4  8.93 #> 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1  9.69"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"view-all-versions-of-a-table","dir":"Articles","previous_headings":"Reading data recipes","what":"View all versions of a table","title":"ducklake Cookbook","text":"","code":"# See all snapshots for the cars table list_table_snapshots(\"cars\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:16:53              1 #> 3           2 2026-02-09 21:16:53              2 #>                                                                 changes #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #>          author                commit_message commit_extra_info #> 2 Data Engineer         Initial car data load              <NA> #> 3 Data Engineer Add km/L metric to cars table              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"read-a-specific-version","dir":"Articles","previous_headings":"Reading data recipes","what":"Read a specific version","title":"ducklake Cookbook","text":"","code":"# Query data as it existed at snapshot 1 get_ducklake_table_version(\"cars\", version = 1) |>   collect()"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"read-data-at-a-specific-timestamp","dir":"Articles","previous_headings":"Reading data recipes","what":"Read data at a specific timestamp","title":"ducklake Cookbook","text":"","code":"# Query data as of a specific time get_ducklake_table_asof(\"cars\", timestamp = \"2024-01-15 10:30:00\") |>   collect()"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"replace-entire-table","dir":"Articles","previous_headings":"Updating data recipes","what":"Replace entire table","title":"ducklake Cookbook","text":"Note: use cases, use replace_table() update tables. creates clean snapshots maintains full versioning. Advanced row-level operations (rows_update, rows_insert, rows_delete) available need granular control, create versioned snapshots.","code":"with_transaction(   get_ducklake_table(\"cars\") |>     mutate(hp_per_cyl = hp / as.numeric(cyl)) |>  # Add derived metric     replace_table(\"cars\"),   author = \"Data Engineer\",   commit_message = \"Add horsepower per cylinder metric\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"list-all-tables","dir":"Articles","previous_headings":"Metadata and versioning recipes","what":"List all tables","title":"ducklake Cookbook","text":"","code":"get_ducklake_table(\"duckdb_tables\") |>   filter(schema_name == \"main\") |>   select(table_name) |>   collect() #> # A tibble: 25 × 1 #>    table_name                            #>    <chr>                                 #>  1 ducklake_column                       #>  2 ducklake_column_mapping               #>  3 ducklake_column_tag                   #>  4 ducklake_data_file                    #>  5 ducklake_delete_file                  #>  6 ducklake_files_scheduled_for_deletion #>  7 ducklake_file_column_stats            #>  8 ducklake_file_partition_value         #>  9 ducklake_inlined_data_tables          #> 10 ducklake_metadata                     #> # ℹ 15 more rows"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"view-all-snapshots","dir":"Articles","previous_headings":"Metadata and versioning recipes","what":"View all snapshots","title":"ducklake Cookbook","text":"","code":"list_table_snapshots() #>   snapshot_id       snapshot_time schema_version #> 1           0 2026-02-09 21:16:53              0 #> 2           1 2026-02-09 21:16:53              1 #> 3           2 2026-02-09 21:16:53              2 #> 4           3 2026-02-09 21:16:53              3 #> 5           4 2026-02-09 21:16:54              4 #> 6           5 2026-02-09 21:16:54              5 #>                                                                 changes #> 1                                                 schemas_created, main #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4             tables_created, tables_inserted_into, main.iris_sample, 3 #> 5          tables_created, tables_inserted_into, main.efficient_cars, 4 #> 6 tables_created, tables_dropped, tables_inserted_into, main.cars, 2, 5 #>          author                     commit_message commit_extra_info #> 1          <NA>                               <NA>              <NA> #> 2 Data Engineer              Initial car data load              <NA> #> 3 Data Engineer      Add km/L metric to cars table              <NA> #> 4 Data Engineer          Load iris sample from CSV              <NA> #> 5  Data Analyst             Load filtered car data              <NA> #> 6 Data Engineer Add horsepower per cylinder metric              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"view-snapshots-for-a-specific-table","dir":"Articles","previous_headings":"Metadata and versioning recipes","what":"View snapshots for a specific table","title":"ducklake Cookbook","text":"","code":"list_table_snapshots(\"cars\")"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"restore-a-table-to-a-previous-version","dir":"Articles","previous_headings":"Metadata and versioning recipes","what":"Restore a table to a previous version","title":"ducklake Cookbook","text":"","code":"# Use time travel to read an old version, then replace the current table with_transaction(   get_ducklake_table_version(\"cars\", version = 1) |>     replace_table(\"cars\"),   author = \"Data Engineer\",   commit_message = \"Restore to version 1\" )  list_table_snapshots(\"cars\")"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"simple-transaction","dir":"Articles","previous_headings":"Transaction recipes","what":"Simple transaction","title":"ducklake Cookbook","text":"","code":"with_transaction(   create_table(my_data, \"my_table\"),   author = \"Your Name\",   commit_message = \"What changed and why\" )"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"multi-step-transaction","dir":"Articles","previous_headings":"Transaction recipes","what":"Multi-step transaction","title":"ducklake Cookbook","text":"","code":"with_transaction({   # All these operations happen atomically   create_table(raw_data, \"raw_table\")      cleaned <- get_ducklake_table(\"raw_table\") |>     filter(!is.na(key_field)) |>     create_table(\"clean_table\")      get_ducklake_table(\"clean_table\") |>     mutate(derived_field = calculate_something(x)) |>     create_table(\"analysis_table\") }, author = \"Data Engineer\", commit_message = \"Full ETL pipeline run\" )"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"manual-transaction-control","dir":"Articles","previous_headings":"Transaction recipes","what":"Manual transaction control","title":"ducklake Cookbook","text":"","code":"# For fine-grained control begin_transaction()  create_table(data1, \"table1\") create_table(data2, \"table2\")  # Commit or rollback commit_transaction(   author = \"Your Name\",   commit_message = \"Manual transaction commit\" )  # Or if something went wrong: # rollback_transaction()"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"preview-query-without-execution","dir":"Articles","previous_headings":"Query optimization recipes","what":"Preview query without execution","title":"ducklake Cookbook","text":"","code":"get_ducklake_table(\"cars\") |>   filter(mpg > 25) |>   mutate(efficient = TRUE) |>   show_ducklake_query()"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"filter-early-for-performance","dir":"Articles","previous_headings":"Query optimization recipes","what":"Filter early for performance","title":"ducklake Cookbook","text":"","code":"# Good: Filter before other operations get_ducklake_table(\"cars\") |>   filter(cyl == 6) |>   mutate(kpl = mpg * 0.425144) |>   head(3) #> # Source:   SQL [?? x 13] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/Rtmps7yUcv/duckplyr/duckplyr1f94478baa40.duckdb] #>     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb   kpl #>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4  8.93 #> 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4  8.93 #> 3  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1  9.10 #> # ℹ 1 more variable: hp_per_cyl <dbl>"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"use-specific-columns","dir":"Articles","previous_headings":"Query optimization recipes","what":"Use specific columns","title":"ducklake Cookbook","text":"","code":"# Good: Select only needed columns get_ducklake_table(\"cars\") |>   select(mpg, cyl, hp) |>   filter(mpg > 25) #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/Rtmps7yUcv/duckplyr/duckplyr1f94478baa40.duckdb] #>     mpg   cyl    hp #>   <dbl> <dbl> <dbl> #> 1  32.4     4    66 #> 2  30.4     4    52 #> 3  33.9     4    65 #> 4  27.3     4    66 #> 5  26       4    91 #> 6  30.4     4   113"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"cleanup","dir":"Articles","previous_headings":"","what":"Cleanup","title":"ducklake Cookbook","text":"","code":"# Detach from the lake detach_ducklake(\"my_lake\")"},{"path":"https://tgerke.github.io/ducklake-r/articles/ducklake.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See also","title":"ducklake Cookbook","text":"Modifying Tables - Detailed guide table modification approaches Transactions - Advanced transaction patterns Time Travel - Comprehensive time travel guide Clinical Trial Data Lake - Complete real-world workflow","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/modifying-tables.html","id":"best-practices-for-table-modifications","dir":"Articles","previous_headings":"","what":"Best Practices for Table Modifications","title":"Modifying Tables with Version Control","text":"data workflows requiring audit trails reproducibility, ducklake offers versioning functions preserve complete history changes.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/modifying-tables.html","id":"recommended-replace_table-with-transactions","dir":"Articles","previous_headings":"Best Practices for Table Modifications","what":"Recommended: replace_table() with transactions","title":"Modifying Tables with Version Control","text":"Use replace_table() wrapped with_transaction() table modifications: approach? Creates snapshots - Every change versioned can time-traveled back Maintains audit trail - Complete history changed, , Enables reproducibility - Recreate analyses point time Supports regulatory compliance - Meets 21 CFR Part 11 requirements GxP work Works dplyr - Natural pipeline syntax filter(), mutate(), select(), etc.","code":"# Modify and replace - creates versioned snapshot with_transaction(   get_ducklake_table(\"my_table\") |>     filter(status == \"active\") |>     mutate(processed = TRUE) |>     replace_table(\"my_table\"),   author = \"Your Name\",   commit_message = \"Mark active records as processed\" )"},{"path":"https://tgerke.github.io/ducklake-r/articles/modifying-tables.html","id":"alternative-rows_-functions-no-versioning","dir":"Articles","previous_headings":"Best Practices for Table Modifications","what":"Alternative: rows_* functions (⚠️ No versioning)","title":"Modifying Tables with Version Control","text":"rows_* functions provide dplyr-style operations create snapshots audit trails: ⚠️ Use : - Working non-GxP environments audit trails required - explicitly want avoid creating new versions workflows, especially requiring reproducibility regulatory compliance, prefer replace_table() maintain complete data lineage.","code":"# These modify tables in-place WITHOUT creating snapshots rows_update(get_ducklake_table(\"my_table\"), updates, by = \"id\") rows_insert(get_ducklake_table(\"my_table\"), new_data, by = \"id\") rows_delete(get_ducklake_table(\"my_table\"), to_delete, by = \"id\")"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/modifying-tables.html","id":"updating-specific-rows-with-replace_table","dir":"Articles","previous_headings":"Examples","what":"Updating specific rows with replace_table()","title":"Modifying Tables with Version Control","text":"","code":"# Update mpg values for specific cars (4-cylinder cars get a 5% efficiency boost) with_transaction(   get_ducklake_table(\"cars\") |>     mutate(       mpg = if_else(cyl == 4, mpg * 1.05, mpg)     ) |>     replace_table(\"cars\"),   author = \"Data Engineer\",   commit_message = \"Update MPG for 4-cylinder vehicles\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Check version history - should show the new snapshot list_table_snapshots(\"cars\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:16:58              1 #> 3           2 2026-02-09 21:16:58              2 #>                                                                 changes #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #>          author                     commit_message commit_extra_info #> 2 Data Engineer              Initial car data load              <NA> #> 3 Data Engineer Update MPG for 4-cylinder vehicles              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/modifying-tables.html","id":"adding-derived-columns","dir":"Articles","previous_headings":"Examples","what":"Adding derived columns","title":"Modifying Tables with Version Control","text":"","code":"# Add new derived columns to existing table with_transaction(   get_ducklake_table(\"cars\") |>     mutate(       hp_per_cyl = hp / cyl,       # Add a new flag column       high_performance = if_else(hp > 200, \"Y\", \"N\")     ) |>     replace_table(\"cars\"),   author = \"Data Engineer\",   commit_message = \"Add HP per cylinder and performance flag\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Verify new columns exist get_ducklake_table(\"cars\") |>   filter(hp > 200) |>   select(hp, cyl, hp_per_cyl, high_performance) #> # Source:   SQL [?? x 4] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpyqzfFH/duckplyr/duckplyr200043645d50.duckdb] #>      hp   cyl hp_per_cyl high_performance #>   <dbl> <dbl>      <dbl> <chr>            #> 1   245     8       30.6 Y                #> 2   205     8       25.6 Y                #> 3   215     8       26.9 Y                #> 4   230     8       28.8 Y                #> 5   245     8       30.6 Y                #> 6   264     8       33   Y                #> 7   335     8       41.9 Y"},{"path":"https://tgerke.github.io/ducklake-r/articles/modifying-tables.html","id":"filtering-rows-with-replace_table","dir":"Articles","previous_headings":"Examples","what":"Filtering rows with replace_table()","title":"Modifying Tables with Version Control","text":"","code":"# Keep only specific rows - creates a versioned snapshot with_transaction(   get_ducklake_table(\"cars\") |>     filter(cyl == 8) |>     replace_table(\"cars\"),   author = \"Data Engineer\",   commit_message = \"Filter to V8 engines only\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Show the filtered table get_ducklake_table(\"cars\") #> # Source:   table<cars> [?? x 13] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpyqzfFH/duckplyr/duckplyr200043645d50.duckdb] #>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb hp_per_cyl #>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>      <dbl> #>  1  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2       21.9 #>  2  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4       30.6 #>  3  16.4     8  276.   180  3.07  4.07  17.4     0     0     3     3       22.5 #>  4  17.3     8  276.   180  3.07  3.73  17.6     0     0     3     3       22.5 #>  5  15.2     8  276.   180  3.07  3.78  18       0     0     3     3       22.5 #>  6  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4       25.6 #>  7  10.4     8  460    215  3     5.42  17.8     0     0     3     4       26.9 #>  8  14.7     8  440    230  3.23  5.34  17.4     0     0     3     4       28.8 #>  9  15.5     8  318    150  2.76  3.52  16.9     0     0     3     2       18.8 #> 10  15.2     8  304    150  3.15  3.44  17.3     0     0     3     2       18.8 #> 11  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4       30.6 #> 12  19.2     8  400    175  3.08  3.84  17.0     0     0     3     2       21.9 #> 13  15.8     8  351    264  4.22  3.17  14.5     0     1     5     4       33   #> 14  15       8  301    335  3.54  3.57  14.6     0     1     5     8       41.9 #> # ℹ 1 more variable: high_performance <chr>  # View version history - old versions still accessible via time travel list_table_snapshots(\"cars\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:16:58              1 #> 3           2 2026-02-09 21:16:58              2 #> 4           3 2026-02-09 21:16:58              3 #> 5           4 2026-02-09 21:16:59              4 #>                                                                 changes #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, 2, 3 #> 5 tables_created, tables_dropped, tables_inserted_into, main.cars, 3, 4 #>          author                           commit_message commit_extra_info #> 2 Data Engineer                    Initial car data load              <NA> #> 3 Data Engineer       Update MPG for 4-cylinder vehicles              <NA> #> 4 Data Engineer Add HP per cylinder and performance flag              <NA> #> 5 Data Engineer                Filter to V8 engines only              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/modifying-tables.html","id":"time-travel-accessing-previous-versions","dir":"Articles","previous_headings":"Examples","what":"Time Travel: Accessing Previous Versions","title":"Modifying Tables with Version Control","text":"","code":"# Get the current version current <- get_ducklake_table(\"cars\") |> collect()  # List all snapshots to see available versions snapshots <- list_table_snapshots(\"cars\") snapshots #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:16:58              1 #> 3           2 2026-02-09 21:16:58              2 #> 4           3 2026-02-09 21:16:58              3 #> 5           4 2026-02-09 21:16:59              4 #>                                                                 changes #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, 2, 3 #> 5 tables_created, tables_dropped, tables_inserted_into, main.cars, 3, 4 #>          author                           commit_message commit_extra_info #> 2 Data Engineer                    Initial car data load              <NA> #> 3 Data Engineer       Update MPG for 4-cylinder vehicles              <NA> #> 4 Data Engineer Add HP per cylinder and performance flag              <NA> #> 5 Data Engineer                Filter to V8 engines only              <NA>  # Access a specific previous version by snapshot_id original_version <- get_ducklake_table_version(   \"cars\",    snapshots$snapshot_id[1] ) |> collect()  # Compare: how many rows changed? nrow(current) #> [1] 14 nrow(original_version) #> [1] 32"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Storage and Backup Management","text":"Understanding DuckLake stores manages data crucial maintaining robust data lake. vignette explains: two-component architecture DuckLake (catalog storage) types files created inspect Best practices choosing storage locations implement backup recovery strategies","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"ducklakes-two-component-architecture","dir":"Articles","previous_headings":"","what":"DuckLake’s Two-Component Architecture","title":"Storage and Backup Management","text":"DuckLake separates data management two distinct components: Catalog (Metadata): database file (DuckDB, SQLite, PostgreSQL) stores metadata tables, snapshots, transactions, data file locations. typically small critically important. Storage (Data Files): directory containing immutable Parquet files hold actual data. DuckLake never modifies existing files—creates new ones. separation provides several benefits: Simplified consistency: Since files never modified, caching replication straightforward Flexible storage options: Store metadata locally data cloud, vice versa Independent backup strategies: component can backed differently based needs","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"storage-options","dir":"Articles","previous_headings":"","what":"Storage Options","title":"Storage and Backup Management","text":"DuckLake works filesystem backend DuckDB supports, including: Local files folders: Fast access, ideal single-machine workflows AWS S3 (S3-compatible services like Cloudflare R2, MinIO) Google Cloud Storage Azure Blob Storage Network-attached storage: NFS, SMB, FUSE-based filesystems","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"storage-patterns","dir":"Articles","previous_headings":"Storage Options","what":"Storage Patterns","title":"Storage and Backup Management","text":"common storage patterns may look: Key considerations: Latency vs. accessibility: Local storage fast shareable; cloud storage accessible higher latency Scalability vs. cost: Object stores scale easily may charge data transfer Security: Consider using DuckLake’s encryption features cloud storage","code":"# Local storage - fastest, but not shared attach_ducklake(   ducklake_name = \"local_lake\",   lake_path = \"~/data/my_ducklake\" )  # Cloud storage - scalable, accessible from anywhere attach_ducklake(   ducklake_name = \"cloud_lake\",   lake_path = \"s3://my-bucket/ducklake\",   data_path = \"s3://my-bucket/ducklake/data\" )  # Hybrid approach - metadata local, data in cloud attach_ducklake(   ducklake_name = \"hybrid_lake\",   lake_path = \"~/data/metadata\",   data_path = \"s3://my-bucket/data\" )"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"inspecting-ducklake-files","dir":"Articles","previous_headings":"","what":"Inspecting DuckLake Files","title":"Storage and Backup Management","text":"Let’s create sample DuckLake explore files generates:","code":"# Create a temporary directory for our demo lake_dir <- file.path(vignette_temp_dir, \"storage_demo\") dir.create(lake_dir, showWarnings = FALSE, recursive = TRUE)  # Install ducklake extension install_ducklake()  # Create and populate a DuckLake attach_ducklake(   ducklake_name = \"demo_lake\",   lake_path = lake_dir )  # Add some data with transactions with_transaction(   create_table(mtcars[1:15, ], \"cars\"),   author = \"Demo User\",   commit_message = \"Initial load\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  with_transaction(   get_ducklake_table(\"cars\") |>     mutate(hp_per_cyl = hp / cyl) |>     replace_table(\"cars\"),   author = \"Demo User\",   commit_message = \"Add hp_per_cyl metric\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  with_transaction(   get_ducklake_table(\"cars\") |>     mutate(mpg_adjusted = if_else(cyl == 4, mpg * 1.1, mpg)) |>     replace_table(\"cars\"),   author = \"Demo User\",   commit_message = \"Add adjusted MPG for 4-cylinder cars\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"catalog-files","dir":"Articles","previous_headings":"Inspecting DuckLake Files","what":"Catalog Files","title":"Storage and Backup Management","text":"catalog single database file containing metadata: catalog files (demo_lake.ducklake .wal) contain metadata tables, snapshots, transactions.","code":"dir_tree(lake_dir) #> /tmp/Rtmpy1ufmJ/storage_backups_vignette/storage_demo #> ├── demo_lake.ducklake #> ├── demo_lake.ducklake.wal #> └── main #>     └── cars #>         ├── ducklake-019c4443-d4b4-7757-9f4c-b7ce0fc7cc40.parquet #>         ├── ducklake-019c4443-d58e-74af-a370-bd57bbd19998.parquet #>         └── ducklake-019c4443-d5db-7869-9411-c7b66b044a81.parquet"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"storage-data-files","dir":"Articles","previous_headings":"Inspecting DuckLake Files","what":"Storage (Data) Files","title":"Storage and Backup Management","text":"Data files stored Parquet format structured directory:","code":"# Data files are organized by schema and table main_dir <- file.path(lake_dir, \"main\")  dir_tree(main_dir, recurse = 2) #> /tmp/Rtmpy1ufmJ/storage_backups_vignette/storage_demo/main #> └── cars #>     ├── ducklake-019c4443-d4b4-7757-9f4c-b7ce0fc7cc40.parquet #>     ├── ducklake-019c4443-d58e-74af-a370-bd57bbd19998.parquet #>     └── ducklake-019c4443-d5db-7869-9411-c7b66b044a81.parquet    # Get details about parquet files parquet_files <- dir_ls(main_dir, recurse = TRUE, regexp = \"\\\\.parquet$\") for (f in parquet_files) {   cat(sprintf(\"  %s (%s bytes)\\n\",                path_file(f),                file.size(f))) } #>   ducklake-019c4443-d4b4-7757-9f4c-b7ce0fc7cc40.parquet (2271 bytes) #>   ducklake-019c4443-d58e-74af-a370-bd57bbd19998.parquet (2462 bytes) #>   ducklake-019c4443-d5db-7869-9411-c7b66b044a81.parquet (2682 bytes)"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"understanding-file-organization","dir":"Articles","previous_headings":"Inspecting DuckLake Files","what":"Understanding File Organization","title":"Storage and Backup Management","text":"table’s data organized schema table, transaction creating new Parquet files: key insight DuckLake never modifies deletes existing Parquet files. change creates new files, preserving complete history time travel queries.","code":"# List all snapshots to see the version history snapshots <- list_table_snapshots(\"cars\") snapshots |>   select(snapshot_id, author, commit_message) #>   snapshot_id    author                       commit_message #> 2           1 Demo User                         Initial load #> 3           2 Demo User                Add hp_per_cyl metric #> 4           3 Demo User Add adjusted MPG for 4-cylinder cars"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"backing-up-the-catalog","dir":"Articles","previous_headings":"Backup Strategies","what":"Backing Up the Catalog","title":"Storage and Backup Management","text":"catalog critical component—maps snapshots data files. Regular backups essential.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"simple-file-copy","dir":"Articles","previous_headings":"Backup Strategies > Backing Up the Catalog","what":"Simple File Copy","title":"Storage and Backup Management","text":"local databases, simplest backup file copy: Important: Transactions committed backup won’t tracked recovering. data exist Parquet files, backup point earlier snapshot. Best practices: Back batch jobs complete streaming/continuous updates, schedule periodic backups Consider using cronR taskscheduleR automated backups","code":"# Create backup directory backup_dir <- file.path(lake_dir, \"backups\") dir.create(backup_dir, showWarnings = FALSE)  # Copy the catalog file to create a backup file.copy(   from = file.path(lake_dir, \"demo_lake.ducklake\"),   to = file.path(backup_dir, \"demo_lake.ducklake\") ) #> [1] TRUE  # Copy the data directory as well dir_copy(   path = file.path(lake_dir, \"main\"),   new_path = file.path(backup_dir, \"main\") )  # Verify the backup was created dir_tree(backup_dir) #> /tmp/Rtmpy1ufmJ/storage_backups_vignette/storage_demo/backups #> ├── demo_lake.ducklake #> └── main #>     └── cars #>         ├── ducklake-019c4443-d4b4-7757-9f4c-b7ce0fc7cc40.parquet #>         ├── ducklake-019c4443-d58e-74af-a370-bd57bbd19998.parquet #>         └── ducklake-019c4443-d5db-7869-9411-c7b66b044a81.parquet  # To use the backup, detach the current lake and attach to the backup # First detach the original detach_ducklake(\"demo_lake\")  # Attach to the backup location attach_ducklake(   ducklake_name = \"demo_lake\",   lake_path = backup_dir )  # Verify you're working with the backup list_table_snapshots(\"cars\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:17:02              1 #> 3           2 2026-02-09 21:17:02              2 #> 4           3 2026-02-09 21:17:02              3 #>                                                                 changes #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, 2, 3 #>      author                       commit_message commit_extra_info #> 2 Demo User                         Initial load              <NA> #> 3 Demo User                Add hp_per_cyl metric              <NA> #> 4 Demo User Add adjusted MPG for 4-cylinder cars              <NA>  # You can switch back to the original by detaching and reattaching detach_ducklake(\"demo_lake\") attach_ducklake(\"demo_lake\", lake_path = lake_dir)"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"backing-up-storage-data-files","dir":"Articles","previous_headings":"Backup Strategies","what":"Backing Up Storage (Data Files)","title":"Storage and Backup Management","text":"Since Parquet files immutable, backing storage straightforward.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"local-storage-backup","dir":"Articles","previous_headings":"Backup Strategies > Backing Up Storage (Data Files)","what":"Local Storage Backup","title":"Storage and Backup Management","text":"","code":"# Use file system tools to copy the entire data directory backup_data_dir <- file.path(lake_dir, \"backups\", \"main_backup\") dir_copy(   path = file.path(lake_dir, \"main\"),   new_path = backup_data_dir )"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"cloud-storage-backup","dir":"Articles","previous_headings":"Backup Strategies > Backing Up Storage (Data Files)","what":"Cloud Storage Backup","title":"Storage and Backup Management","text":"cloud storage, use provider-specific mechanisms: AWS S3: - Cross-bucket replication (copies different bucket automatically) - AWS Backup service (scheduled backups within bucket) - S3 versioning (keeps previous versions objects) Google Cloud Storage: - Cross-bucket replication - Backup DR service - Object versioning soft deletes using cross-bucket replication, update data path:","code":"# Original attach_ducklake(   ducklake_name = \"prod_lake\",   data_path = \"s3://original-bucket/data\" )  # After recovery from replicated bucket attach_ducklake(   ducklake_name = \"prod_lake\",   data_path = \"s3://backup-bucket/data\" )"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"recovering-from-catalog-backup","dir":"Articles","previous_headings":"Recovery Procedures","what":"Recovering from Catalog Backup","title":"Storage and Backup Management","text":"catalog corrupted lost:","code":"# Restore from backup by copying the backup file file.copy(   from = file.path(lake_dir, \"backups\",                     paste0(\"demo_lake_backup_\", Sys.Date(), \".ducklake\")),   to = file.path(lake_dir, \"demo_lake.ducklake\"),   overwrite = TRUE )  # Reattach to the restored database attach_ducklake(\"demo_lake\", lake_path = lake_dir)  # Verify recovery by listing snapshots list_table_snapshots(\"cars\")"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"recovering-from-data-file-loss","dir":"Articles","previous_headings":"Recovery Procedures","what":"Recovering from Data File Loss","title":"Storage and Backup Management","text":"data files lost catalog intact:","code":"# Restore data files from backup dir_copy(   path = backup_data_dir,   new_path = file.path(lake_dir, \"main\"),   overwrite = TRUE )  # DuckLake will automatically reconnect to the restored files # since the catalog maintains the file paths"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"maintenance-considerations","dir":"Articles","previous_headings":"","what":"Maintenance Considerations","title":"Storage and Backup Management","text":"planning backups, coordinate maintenance operations: Compaction (merging adjacent files): Run backups ensure consistent file layout Cleanup (removing obsolete files): Run backups avoid backing unnecessary files","code":"# Recommended backup sequence  # 1. Run maintenance operations (if needed) # See maintenance vignettes for details  # 2. Ensure all transactions are committed # (no pending work)  # 3. Back up catalog dir.create(file.path(lake_dir, \"backups\"), showWarnings = FALSE) file.copy(   from = file.path(lake_dir, \"demo_lake.ducklake\"),   to = file.path(lake_dir, \"backups\",                   paste0(\"backup_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".ducklake\")) )  # 4. Back up data files dir_copy(   path = file.path(lake_dir, \"main\"),   new_path = file.path(lake_dir, \"backups\", \"main_latest\") )"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"complete-backup-example","dir":"Articles","previous_headings":"","what":"Complete Backup Example","title":"Storage and Backup Management","text":"DuckLake provides convenient backup_ducklake() function creating timestamped backups: backup_ducklake() function: - Creates timestamped backup directory - Copies catalog database file - Copies data files main/ directory - Returns backup directory path reference","code":"# Create a complete backup with timestamp backup_dir <- backup_ducklake(   ducklake_name = \"demo_lake\",   lake_path = lake_dir,   backup_path = file.path(lake_dir, \"backups\") ) #> Catalog backed up successfully #> Data files backed up successfully #> Backup completed: /tmp/Rtmpy1ufmJ/storage_backups_vignette/storage_demo/backups/backup_20260209_211703  # The function returns the backup directory path print(backup_dir) #> [1] \"/tmp/Rtmpy1ufmJ/storage_backups_vignette/storage_demo/backups/backup_20260209_211703\""},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"cleanup","dir":"Articles","previous_headings":"","what":"Cleanup","title":"Storage and Backup Management","text":"","code":"# Detach the demo lake detach_ducklake(\"demo_lake\")  # Clean up temporary files unlink(lake_dir, recursive = TRUE)"},{"path":"https://tgerke.github.io/ducklake-r/articles/storage-and-backups.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Storage and Backup Management","text":"Key takeaways managing DuckLake storage backups: Understand architecture: Catalog (metadata) storage (data) separate components Choose storage wisely: Balance latency, scalability, cost, accessibility Files immutable: DuckLake never modifies existing Parquet files Back regularly: Catalog backups critical; back batch jobs Coordinate maintenance: Run compaction cleanup backups Test recovery procedures: Ensure can actually restore backups production systems, consider: Automated backup scheduling (using cronR taskscheduleR) Multiple backup locations (local cloud) Testing recovery procedures regularly Monitoring backup success storage usage Version control catalog schema changes","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Time Travel Queries","text":"DuckLake’s time travel capabilities provide powerful audit trail data, enabling : View data existed specific point time Query specific versions tables Restore tables previous states Track complete history changes Meet regulatory compliance requirements functionality especially valuable domains data provenance reproducibility critical, clinical trials, financial reporting, scientific research.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"setting-up-the-data-lake","dir":"Articles","previous_headings":"","what":"Setting Up the Data Lake","title":"Time Travel Queries","text":"’ll start creating new DuckLake loading mtcars dataset. ’ll make several modifications demonstrate time travel functionality.","code":"# Install the ducklake extension (required once per system) install_ducklake()  # Create or attach to a data lake attach_ducklake(   ducklake_name = \"time_travel_demo\",   lake_path = vignette_temp_dir )  # Create initial table with the mtcars dataset with_transaction(   create_table(mtcars, \"cars\"),   author = \"Data Engineer\",   commit_message = \"Initial load of mtcars dataset\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Verify the table was created get_ducklake_table(\"cars\") |>   select(mpg, cyl, hp, wt) |>   head() #> # Source:   SQL [?? x 4] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #>     mpg   cyl    hp    wt #>   <dbl> <dbl> <dbl> <dbl> #> 1  21       6   110  2.62 #> 2  21       6   110  2.88 #> 3  22.8     4    93  2.32 #> 4  21.4     6   110  3.22 #> 5  18.7     8   175  3.44 #> 6  18.1     6   105  3.46"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"making-changes-over-time","dir":"Articles","previous_headings":"","what":"Making Changes Over Time","title":"Time Travel Queries","text":"Let’s make several changes data create version history can explore.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"version-1-initial-data","dir":"Articles","previous_headings":"Making Changes Over Time","what":"Version 1: Initial data","title":"Time Travel Queries","text":"already initial dataset. Let’s check current state:","code":"get_ducklake_table(\"cars\") |>   summarise(     n_cars = n(),     avg_mpg = mean(mpg, na.rm = TRUE),     avg_hp = mean(hp, na.rm = TRUE)   ) #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #>   n_cars avg_mpg avg_hp #>    <dbl>   <dbl>  <dbl> #> 1     32    20.1   147."},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"version-2-update-fuel-efficiency-data","dir":"Articles","previous_headings":"Making Changes Over Time","what":"Version 2: Update fuel efficiency data","title":"Time Travel Queries","text":"Suppose discover fuel efficiency measurements need adjusted vehicles:","code":"# Update mpg for high-performance cars (5% reduction) with_transaction(   get_ducklake_table(\"cars\") |>     mutate(mpg = if_else(hp > 200, mpg * 0.95, mpg)) |>     replace_table(\"cars\"),   author = \"Data Analyst\",   commit_message = \"Adjust MPG for high-performance vehicles\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Check the updated averages get_ducklake_table(\"cars\") |>   summarise(     n_cars = n(),     avg_mpg = mean(mpg, na.rm = TRUE),     avg_hp = mean(hp, na.rm = TRUE)   ) #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #>   n_cars avg_mpg avg_hp #>    <dbl>   <dbl>  <dbl> #> 1     32    19.9   147."},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"version-3-add-efficiency-classification","dir":"Articles","previous_headings":"Making Changes Over Time","what":"Version 3: Add efficiency classification","title":"Time Travel Queries","text":"Let’s add new categorical variable classify cars fuel efficiency:","code":"with_transaction(   get_ducklake_table(\"cars\") |>     mutate(       efficiency_class = case_when(         mpg >= 25 ~ \"High\",         mpg >= 20 ~ \"Medium\",         TRUE ~ \"Low\"       )     ) |>     replace_table(\"cars\"),   author = \"Data Analyst\",   commit_message = \"Add efficiency classification\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # View the new classification get_ducklake_table(\"cars\") |>   count(efficiency_class) |>   arrange(desc(n)) #> # Source:     SQL [?? x 2] #> # Database:   DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #> # Ordered by: desc(n) #>   efficiency_class     n #>   <chr>            <dbl> #> 1 Low                 18 #> 2 Medium               8 #> 3 High                 6"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"version-4-correct-an-error","dir":"Articles","previous_headings":"Making Changes Over Time","what":"Version 4: Correct an error","title":"Time Travel Queries","text":"Suppose realize efficiency classification thresholds wrong need corrected:","code":"with_transaction(   get_ducklake_table(\"cars\") |>     mutate(       efficiency_class = case_when(         mpg >= 30 ~ \"High\",         mpg >= 20 ~ \"Medium\",         TRUE ~ \"Low\"       )     ) |>     replace_table(\"cars\"),   author = \"Senior Analyst\",   commit_message = \"Correct efficiency classification thresholds\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # View the corrected classification get_ducklake_table(\"cars\") |>   count(efficiency_class) |>   arrange(desc(n)) #> # Source:     SQL [?? x 2] #> # Database:   DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #> # Ordered by: desc(n) #>   efficiency_class     n #>   <chr>            <dbl> #> 1 Low                 18 #> 2 Medium              10 #> 3 High                 4"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"exploring-version-history","dir":"Articles","previous_headings":"","what":"Exploring Version History","title":"Time Travel Queries","text":"Now history changes, let’s explore time travel functionality.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"list-all-snapshots","dir":"Articles","previous_headings":"Exploring Version History","what":"List all snapshots","title":"Time Travel Queries","text":"","code":"# View all available versions of the table snapshots <- list_table_snapshots(\"cars\") snapshots #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:17:07              1 #> 3           2 2026-02-09 21:17:08              2 #> 4           3 2026-02-09 21:17:08              3 #> 5           4 2026-02-09 21:17:08              4 #>                                                                 changes #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, 2, 3 #> 5 tables_created, tables_dropped, tables_inserted_into, main.cars, 3, 4 #>           author                               commit_message commit_extra_info #> 2  Data Engineer               Initial load of mtcars dataset              <NA> #> 3   Data Analyst     Adjust MPG for high-performance vehicles              <NA> #> 4   Data Analyst                Add efficiency classification              <NA> #> 5 Senior Analyst Correct efficiency classification thresholds              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"query-a-specific-version","dir":"Articles","previous_headings":"Exploring Version History","what":"Query a specific version","title":"Time Travel Queries","text":"Let’s look version 2, added efficiency classification: Compare version 3, classification:","code":"# Get version 2 (after MPG adjustment, before classification) get_ducklake_table_version(\"cars\", version = 2) |>   select(mpg, cyl, hp, wt) |>   head() #> # Source:   SQL [?? x 4] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #>     mpg   cyl    hp    wt #>   <dbl> <dbl> <dbl> <dbl> #> 1  21       6   110  2.62 #> 2  21       6   110  2.88 #> 3  22.8     4    93  2.32 #> 4  21.4     6   110  3.22 #> 5  18.7     8   175  3.44 #> 6  18.1     6   105  3.46  # Notice: no efficiency_class column yet # Get version 3 (with initial classification) get_ducklake_table_version(\"cars\", version = 3) |>   select(mpg, efficiency_class) |>   count(efficiency_class) #> # Source:   SQL [?? x 2] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #>   efficiency_class     n #>   <chr>            <dbl> #> 1 Medium               8 #> 2 Low                 18 #> 3 High                 6"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"query-data-as-of-a-specific-timestamp","dir":"Articles","previous_headings":"Exploring Version History","what":"Query data as of a specific timestamp","title":"Time Travel Queries","text":"can also query data existed point time:","code":"# Get the timestamp from version 2 version2_timestamp <- snapshots |>   filter(schema_version == 2) |>   pull(snapshot_time)  # Query data as it existed at that time # Note: Add 1 second to ensure we query AFTER the snapshot was created get_ducklake_table_asof(\"cars\", version2_timestamp + 1) |>   summarise(     avg_mpg = mean(mpg, na.rm = TRUE)   ) #> # Source:   SQL [?? x 1] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpGWIuIs/duckplyr/duckplyr20e62eefd211.duckdb] #>   avg_mpg #>     <dbl> #> 1    19.9"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"comparing-versions","dir":"Articles","previous_headings":"","what":"Comparing Versions","title":"Time Travel Queries","text":"One powerful use case comparing different versions understand changed:","code":"# Get MPG values from version 1 (original) and version 2 (after adjustment) original <- get_ducklake_table_version(\"cars\", version = 1) |>   select(mpg) |>   collect() |>   mutate(version = \"Original\")  adjusted <- get_ducklake_table_version(\"cars\", version = 2) |>   select(mpg) |>   collect() |>   mutate(version = \"Adjusted\")  # Combine and compare bind_rows(original, adjusted) |>   group_by(version) |>   summarise(     avg_mpg = mean(mpg, na.rm = TRUE),     min_mpg = min(mpg),     max_mpg = max(mpg)   ) #> # A tibble: 2 × 4 #>   version  avg_mpg min_mpg max_mpg #>   <chr>      <dbl>   <dbl>   <dbl> #> 1 Adjusted    19.9    9.88    33.9 #> 2 Original    20.1   10.4     33.9"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"restoring-previous-versions","dir":"Articles","previous_headings":"","what":"Restoring Previous Versions","title":"Time Travel Queries","text":"need undo changes, can restore table previous version reading version replacing current table: restoring, can see efficiency_class column longer present. new snapshot created restoration:","code":"# Let's say we want to go back to version 2 (before adding classifications) # We restore by reading version 2 and replacing the current table with_transaction(   get_ducklake_table_version(\"cars\", version = 2) |>     replace_table(\"cars\"),   author = \"Senior Analyst\",    commit_message = \"Restore to version 2 (before efficiency classification)\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Verify the restoration - efficiency_class column should be gone get_ducklake_table(\"cars\") |> colnames() #>  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\" #> [11] \"carb\" list_table_snapshots(\"cars\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:17:07              1 #> 3           2 2026-02-09 21:17:08              2 #> 4           3 2026-02-09 21:17:08              3 #> 5           4 2026-02-09 21:17:08              4 #> 6           5 2026-02-09 21:17:09              5 #>                                                                 changes #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, 2, 3 #> 5 tables_created, tables_dropped, tables_inserted_into, main.cars, 3, 4 #> 6 tables_created, tables_dropped, tables_inserted_into, main.cars, 4, 5 #>           author                                          commit_message #> 2  Data Engineer                          Initial load of mtcars dataset #> 3   Data Analyst                Adjust MPG for high-performance vehicles #> 4   Data Analyst                           Add efficiency classification #> 5 Senior Analyst            Correct efficiency classification thresholds #> 6 Senior Analyst Restore to version 2 (before efficiency classification) #>   commit_extra_info #> 2              <NA> #> 3              <NA> #> 4              <NA> #> 5              <NA> #> 6              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"use-cases-for-time-travel","dir":"Articles","previous_headings":"","what":"Use Cases for Time Travel","title":"Time Travel Queries","text":"Time travel functionality particularly valuable : Regulatory Compliance: Maintain complete audit trails datasets used regulatory submissions (e.g., clinical trials, financial reporting) Reproducibility: Recreate analyses exactly run specific points time Data Recovery: Restore accidentally modified deleted data Change Tracking: Understand data quality issues introduced Reporting: Generate historical reports using data existed specific time points Collaboration: Allow team members reference specific versions shared datasets Debugging: Identify unexpected changes occurred data pipeline","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/time-travel.html","id":"metadata-and-audit-information","dir":"Articles","previous_headings":"","what":"Metadata and Audit Information","title":"Time Travel Queries","text":"snapshot includes metadata created changes made. list_table_snapshots() function provides complete audit trail: complete audit trail ensures can always answer questions like: changes made? made? version table ? data change? can also access metadata tables DuckLake:","code":"# Get detailed snapshot history with all metadata snapshot_history <- list_table_snapshots(\"cars\") snapshot_history |>   select(snapshot_id, snapshot_time, author, commit_message) #>   snapshot_id       snapshot_time         author #> 2           1 2026-02-09 21:17:07  Data Engineer #> 3           2 2026-02-09 21:17:08   Data Analyst #> 4           3 2026-02-09 21:17:08   Data Analyst #> 5           4 2026-02-09 21:17:08 Senior Analyst #> 6           5 2026-02-09 21:17:09 Senior Analyst #>                                            commit_message #> 2                          Initial load of mtcars dataset #> 3                Adjust MPG for high-performance vehicles #> 4                           Add efficiency classification #> 5            Correct efficiency classification thresholds #> 6 Restore to version 2 (before efficiency classification) # View metadata for all tables all_snapshots <- list_table_snapshots() all_snapshots |>   select(snapshot_id, snapshot_time, changes) |>   head(10) #>   snapshot_id       snapshot_time #> 1           0 2026-02-09 21:17:07 #> 2           1 2026-02-09 21:17:07 #> 3           2 2026-02-09 21:17:08 #> 4           3 2026-02-09 21:17:08 #> 5           4 2026-02-09 21:17:08 #> 6           5 2026-02-09 21:17:09 #>                                                                 changes #> 1                                                 schemas_created, main #> 2                    tables_created, tables_inserted_into, main.cars, 1 #> 3 tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, 2, 3 #> 5 tables_created, tables_dropped, tables_inserted_into, main.cars, 3, 4 #> 6 tables_created, tables_dropped, tables_inserted_into, main.cars, 4, 5"},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Working with Transactions","text":"Transactions essential maintaining data integrity making multiple related changes data lake. DuckLake provides full ACID (Atomicity, Consistency, Isolation, Durability) transaction support, ensuring either operations succeed none . DuckLake offers two approaches working transactions: with_transaction() (Recommended): modern, R-idiomatic approach automatically handles errors rollbacks begin_transaction() / commit_transaction() / rollback_transaction(): Manual transaction control advanced use cases vignette demonstrates approaches explains use one.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"setup-loading-initial-data","dir":"Articles","previous_headings":"","what":"Setup: Loading Initial Data","title":"Working with Transactions","text":"’ll use mtcars dataset throughout vignette demonstrate transaction workflows.","code":"# Load initial data with_transaction(   create_table(mtcars, \"cars\"),   author = \"Tutorial\",   commit_message = \"Initial load of mtcars dataset\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # View the data get_ducklake_table(\"cars\") |>   select(mpg, cyl, hp, wt) |>   head() #> # Source:   SQL [?? x 4] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpjNMjPV/duckplyr/duckplyr21552ff144e9.duckdb] #>     mpg   cyl    hp    wt #>   <dbl> <dbl> <dbl> <dbl> #> 1  21       6   110  2.62 #> 2  21       6   110  2.88 #> 3  22.8     4    93  2.32 #> 4  21.4     6   110  3.22 #> 5  18.7     8   175  3.44 #> 6  18.1     6   105  3.46"},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"approach-1-with_transaction-recommended","dir":"Articles","previous_headings":"","what":"Approach 1: with_transaction() (Recommended)","title":"Working with Transactions","text":"with_transaction() function provides automatic error handling cleanup, similar withr::with_*() pattern used throughout R ecosystem. recommended approach use cases.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"why-use-with_transaction","dir":"Articles","previous_headings":"Approach 1: with_transaction() (Recommended)","what":"Why use with_transaction()?","title":"Working with Transactions","text":"Automatic rollback error: operation fails, changes automatically rolled back Cleaner code: need manually call begin_transaction() commit_transaction() Built-metadata support: Easily add author commit messages Safer: Prevents accidentally leaving transactions open R-idiomatic: Follows familiar patterns packages like withr","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"single-operation-with-metadata","dir":"Articles","previous_headings":"Approach 1: with_transaction() (Recommended)","what":"Single Operation with Metadata","title":"Working with Transactions","text":"","code":"# Add a new column with automatic metadata tracking with_transaction(   get_ducklake_table(\"cars\") |>     mutate(kpl = mpg * 0.425144) |>     replace_table(\"cars\"),   author = \"Data Team\",   commit_message = \"Add kilometers per liter column\" ) #> Transaction started #> Transaction committed #> Snapshot metadata updated  # Verify the change get_ducklake_table(\"cars\") |>   select(mpg, kpl) |>   head() #> # Source:   SQL [?? x 2] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpjNMjPV/duckplyr/duckplyr21552ff144e9.duckdb] #>     mpg   kpl #>   <dbl> <dbl> #> 1  21    8.93 #> 2  21    8.93 #> 3  22.8  9.69 #> 4  21.4  9.10 #> 5  18.7  7.95 #> 6  18.1  7.70"},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"multiple-operations-in-a-single-transaction","dir":"Articles","previous_headings":"Approach 1: with_transaction() (Recommended)","what":"Multiple Operations in a Single Transaction","title":"Working with Transactions","text":"can group multiple operations together wrapping curly braces:","code":"# Multiple related changes in one atomic transaction with_transaction({   # Add efficiency rating   get_ducklake_table(\"cars\") |>     mutate(       efficiency = case_when(         mpg >= 25 ~ \"high\",         mpg >= 20 ~ \"medium\",         TRUE ~ \"low\"       )     ) |>     replace_table(\"cars\")      # Create a summary table   get_ducklake_table(\"cars\") |>     group_by(cyl) |>     summarize(       avg_mpg = mean(mpg, na.rm = TRUE),       avg_hp = mean(hp, na.rm = TRUE),       count = n()     ) |>     create_table(\"cars_summary\") }, author = \"Data Team\", commit_message = \"Add efficiency ratings and summary table\") #> Transaction started #> Transaction committed #> Snapshot metadata updated  # View results get_ducklake_table(\"cars\") |>   select(mpg, cyl, efficiency) |>   head() #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpjNMjPV/duckplyr/duckplyr21552ff144e9.duckdb] #>     mpg   cyl efficiency #>   <dbl> <dbl> <chr>      #> 1  21       6 medium     #> 2  21       6 medium     #> 3  22.8     4 medium     #> 4  21.4     6 medium     #> 5  18.7     8 low        #> 6  18.1     6 low  get_ducklake_table(\"cars_summary\") |>   collect() #> # A tibble: 3 × 4 #>     cyl avg_mpg avg_hp count #>   <dbl>   <dbl>  <dbl> <dbl> #> 1     4    26.7   82.6    11 #> 2     6    19.7  122.      7 #> 3     8    15.1  209.     14"},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"automatic-rollback-on-error","dir":"Articles","previous_headings":"Approach 1: with_transaction() (Recommended)","what":"Automatic Rollback on Error","title":"Working with Transactions","text":"One key benefits with_transaction() automatic error handling:","code":"# This transaction will fail and automatically rollback tryCatch(   with_transaction({     # This will succeed     get_ducklake_table(\"cars\") |>       mutate(test_column = \"temporary\") |>       replace_table(\"cars\")          # This will fail     stop(\"Simulated error - something went wrong!\")   }, author = \"Data Team\", commit_message = \"This will be rolled back\"),   error = function(e) {     message(\"Transaction automatically rolled back: \", e$message)   } ) #> Transaction started #> Transaction rolled back #> Transaction automatically rolled back: Transaction rolled back due to error: Simulated error - something went wrong!  # Verify that test_column was NOT added (transaction was rolled back) get_ducklake_table(\"cars\") |>   colnames() #>  [1] \"mpg\"        \"cyl\"        \"disp\"       \"hp\"         \"drat\"       #>  [6] \"wt\"         \"qsec\"       \"vs\"         \"am\"         \"gear\"       #> [11] \"carb\"       \"kpl\"        \"efficiency\"  # View all versioned changes list_table_snapshots(\"cars\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:17:12              1 #> 3           2 2026-02-09 21:17:12              2 #> 4           3 2026-02-09 21:17:12              3 #>                                                                                       changes #> 2                                          tables_created, tables_inserted_into, main.cars, 1 #> 3                       tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, main.cars_summary, 2, 3, 4 #>      author                           commit_message commit_extra_info #> 2  Tutorial           Initial load of mtcars dataset              <NA> #> 3 Data Team          Add kilometers per liter column              <NA> #> 4 Data Team Add efficiency ratings and summary table              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"approach-2-manual-transaction-control","dir":"Articles","previous_headings":"","what":"Approach 2: Manual Transaction Control","title":"Working with Transactions","text":"advanced use cases need explicit control transaction boundaries, DuckLake provides manual transaction functions.","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"when-to-use-manual-transactions","dir":"Articles","previous_headings":"Approach 2: Manual Transaction Control","what":"When to use manual transactions?","title":"Working with Transactions","text":"Interactive workflows: want inspect data operations committing Conditional commits: commit logic depends runtime conditions Long-running transactions: need fine-grained control transaction lifecycle Legacy code: migrating transaction systems","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"basic-manual-transaction-workflow","dir":"Articles","previous_headings":"Approach 2: Manual Transaction Control","what":"Basic Manual Transaction Workflow","title":"Working with Transactions","text":"","code":"# Start a transaction begin_transaction() #> Transaction started  # Make changes get_ducklake_table(\"cars\") |>   filter(cyl == 4) |>   mutate(weight_kg = wt * 453.592) |>   replace_table(\"cars\")  # Commit the changes with metadata commit_transaction(   author = \"Data Team\",   commit_message = \"Add weight in kg for 4-cylinder cars\" ) #> Transaction committed #> Snapshot metadata updated  # Verify changes get_ducklake_table(\"cars\") |>   filter(cyl == 4) |>   select(wt, weight_kg) |>   head() #> # Source:   SQL [?? x 2] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpjNMjPV/duckplyr/duckplyr21552ff144e9.duckdb] #>      wt weight_kg #>   <dbl>     <dbl> #> 1  2.32     1052. #> 2  3.19     1447. #> 3  3.15     1429. #> 4  2.2       998. #> 5  1.62      733. #> 6  1.84      832."},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"manual-rollback","dir":"Articles","previous_headings":"Approach 2: Manual Transaction Control","what":"Manual Rollback","title":"Working with Transactions","text":"Sometimes may want inspect data deciding whether commit:","code":"# Start a transaction begin_transaction() #> Transaction started  # Make a test change get_ducklake_table(\"cars\") |>   mutate(test_flag = TRUE) |>   replace_table(\"cars\")  # Check the result test_result <- get_ducklake_table(\"cars\") |>   select(mpg, test_flag) |>   head() |>   collect()  print(test_result) #> # A tibble: 6 × 2 #>     mpg test_flag #>   <dbl> <lgl>     #> 1  22.8 TRUE      #> 2  24.4 TRUE      #> 3  22.8 TRUE      #> 4  32.4 TRUE      #> 5  30.4 TRUE      #> 6  33.9 TRUE  # Decide to rollback rollback_transaction() #> Transaction rolled back  # Verify the change was NOT applied \"test_flag\" %in% colnames(get_ducklake_table(\"cars\")) #> [1] FALSE  # View all versioned changes list_table_snapshots(\"cars\") #>   snapshot_id       snapshot_time schema_version #> 2           1 2026-02-09 21:17:12              1 #> 3           2 2026-02-09 21:17:12              2 #> 4           3 2026-02-09 21:17:12              3 #> 5           4 2026-02-09 21:17:13              4 #>                                                                                       changes #> 2                                          tables_created, tables_inserted_into, main.cars, 1 #> 3                       tables_created, tables_dropped, tables_inserted_into, main.cars, 1, 2 #> 4 tables_created, tables_dropped, tables_inserted_into, main.cars, main.cars_summary, 2, 3, 4 #> 5                       tables_created, tables_dropped, tables_inserted_into, main.cars, 4, 5 #>      author                           commit_message commit_extra_info #> 2  Tutorial           Initial load of mtcars dataset              <NA> #> 3 Data Team          Add kilometers per liter column              <NA> #> 4 Data Team Add efficiency ratings and summary table              <NA> #> 5 Data Team     Add weight in kg for 4-cylinder cars              <NA>"},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"setting-metadata-separately","dir":"Articles","previous_headings":"Approach 2: Manual Transaction Control","what":"Setting Metadata Separately","title":"Working with Transactions","text":"manual transactions, can also set metadata committing:","code":"# Start transaction begin_transaction() #> Transaction started  # Make changes get_ducklake_table(\"cars\") |>   mutate(hp_per_liter = hp / (cyl * 0.5)) |>   replace_table(\"cars\")  # Commit commit_transaction() #> Transaction committed  # Add metadata separately set_snapshot_metadata(   ducklake_name = \"my_ducklake\",   author = \"Performance Team\",   commit_message = \"Add horsepower per liter metric\" ) #> Warning in value[[3L]](cond): Could not update snapshot metadata: Binder Error: Catalog \"__ducklake_metadata_my_ducklake\" does not exist! #> ℹ Context: rapi_prepare #> ℹ Error type: BINDER  # Verify get_ducklake_table(\"cars\") |>   select(hp, cyl, hp_per_liter) |>   head() #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [unknown@Linux 6.11.0-1018-azure:R 4.5.2//tmp/RtmpjNMjPV/duckplyr/duckplyr21552ff144e9.duckdb] #>      hp   cyl hp_per_liter #>   <dbl> <dbl>        <dbl> #> 1    93     4         46.5 #> 2    62     4         31   #> 3    95     4         47.5 #> 4    66     4         33   #> 5    52     4         26   #> 6    65     4         32.5"},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"viewing-transaction-history","dir":"Articles","previous_headings":"","what":"Viewing Transaction History","title":"Working with Transactions","text":"Regardless approach use, transactions tracked complete metadata:","code":"# View recent transaction history list_table_snapshots(\"cars\") |>   select(snapshot_id, snapshot_time, author, commit_message) |>   tail(5) #>   snapshot_id       snapshot_time    author #> 2           1 2026-02-09 21:17:12  Tutorial #> 3           2 2026-02-09 21:17:12 Data Team #> 4           3 2026-02-09 21:17:12 Data Team #> 5           4 2026-02-09 21:17:13 Data Team #> 6           5 2026-02-09 21:17:13      <NA> #>                             commit_message #> 2           Initial load of mtcars dataset #> 3          Add kilometers per liter column #> 4 Add efficiency ratings and summary table #> 5     Add weight in kg for 4-cylinder cars #> 6                                     <NA>"},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"Working with Transactions","text":"Default with_transaction(): Use standard workflows Always add metadata: Include author commit_message audit trails Keep transactions focused: Group related changes, avoid overly long transactions Handle errors gracefully: using manual transactions, always use tryCatch() ensure rollback Test rollback behavior: Verify error handling works correctly","code":""},{"path":"https://tgerke.github.io/ducklake-r/articles/transactions.html","id":"key-concepts-summary","dir":"Articles","previous_headings":"","what":"Key Concepts Summary","title":"Working with Transactions","text":"with_transaction(expr, author, commit_message): Modern, automatic transaction handling (recommended) begin_transaction(): Start manual transaction commit_transaction(author, commit_message): Apply changes manual transaction rollback_transaction(): Discard changes manual transaction set_snapshot_metadata(): Add metadata snapshots committing Transactions ensure data integrity provide complete audit trails changes DuckLake.","code":""},{"path":"https://tgerke.github.io/ducklake-r/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Travis Gerke. Author, maintainer.","code":""},{"path":"https://tgerke.github.io/ducklake-r/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gerke T (2026). ducklake: Interact DuckLake R. R package version 0.1.0, https://tgerke.github.io/ducklake-r/.","code":"@Manual{,   title = {ducklake: Interact with DuckLake from R},   author = {Travis Gerke},   year = {2026},   note = {R package version 0.1.0},   url = {https://tgerke.github.io/ducklake-r/}, }"},{"path":"https://tgerke.github.io/ducklake-r/index.html","id":"ducklake-","dir":"","previous_headings":"","what":"ducklake: Interact with DuckLake from R","title":"ducklake: Interact with DuckLake from R","text":"ducklake R package brings versioned data lake infrastructure data-intensive workflows. Built DuckDB DuckLake, provides ACID transactions, automatic versioning, time travel queries, complete audit trails.","code":""},{"path":"https://tgerke.github.io/ducklake-r/index.html","id":"why-ducklake","dir":"","previous_headings":"","what":"Why DuckLake?","title":"ducklake: Interact with DuckLake from R","text":"Many industries rely flat-file workflows (CSV, XPT, Excel, etc.) create significant data management challenges: Disconnected flat files: Related datasets stored separate files despite inherently relational Lost audit trails: automatic tracking changed Version control gaps: Multiple dataset versions scattered across folders unclear provenance Reproducibility issues: Inability recreate analyses specific time points Collaboration friction: Multiple analysts working different versions data Compliance challenges: Difficulty demonstrating data integrity audit trails regulated industries DuckLake solves problems implementing versioned data lake architecture : Preserves relational structure related datasets Automatically versions every data change timestamps metadata Enables time travel recreate analyses exactly run Provides complete audit trails author attribution commit messages Supports layered architecture (bronze/silver/gold) data lineage raw analysis-ready Allows multiple team members collaborate safely shared data","code":""},{"path":"https://tgerke.github.io/ducklake-r/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"ducklake: Interact with DuckLake from R","text":"Install development version ducklake :","code":"pak::pak(\"tgerke/ducklake-r\")"},{"path":"https://tgerke.github.io/ducklake-r/index.html","id":"quick-example-layered-data-workflow","dir":"","previous_headings":"","what":"Quick example: Layered data workflow","title":"ducklake: Interact with DuckLake from R","text":"","code":"library(ducklake) library(dplyr)  # Install the ducklake extension (requires DuckDB v1.3.0 or higher) install_ducklake()  # Create a data lake in a temporary directory attach_ducklake(\"my_data_lake\", lake_path = tempdir())  # Bronze layer: Load raw data exactly as received with_transaction(   create_table(mtcars, \"vehicles_raw\"),   author = \"Data Engineer\",   commit_message = \"Initial load of raw vehicle data\" )  # Silver layer: Apply cleaning transformations with_transaction(   get_ducklake_table(\"vehicles_raw\") |>     mutate(cyl = as.character(cyl)) |>     create_table(\"vehicles_clean\"),   author = \"Data Engineer\",    commit_message = \"Clean and standardize vehicle data\" )  # Gold layer: Create analysis dataset with business logic with_transaction(   get_ducklake_table(\"vehicles_clean\") |>     mutate(       efficiency = case_when(         mpg < 15 ~ \"Low\",         mpg < 25 ~ \"Medium\",         TRUE ~ \"High\"       )     ) |>     create_table(\"vehicles_analysis\"),   author = \"Data Analyst\",   commit_message = \"Create analysis-ready dataset with efficiency categories\" )  # Update the silver layer with additional transformations with_transaction(   get_ducklake_table(\"vehicles_clean\") |>     mutate(gear = as.integer(gear)) |>     replace_table(\"vehicles_clean\"),   author = \"Data Engineer\",   commit_message = \"Add gear type conversion to silver layer\" )  # View the analysis dataset get_ducklake_table(\"vehicles_analysis\") |>   select(mpg, cyl, efficiency) |>   head(3) #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [tgerke@Darwin 23.6.0:R 4.5.2//private/var/folders/b7/664jmq55319dcb7y4jdb39zr0000gq/T/RtmpOkTzSg/duckplyr/duckplyr2e58131c620d.duckdb] #>     mpg cyl   efficiency #>   <dbl> <chr> <chr>      #> 1  21   6.0   Medium     #> 2  21   6.0   Medium     #> 3  22.8 4.0   Medium  # View complete audit trail across all layers with author and commit messages list_table_snapshots() #>   snapshot_id       snapshot_time schema_version #> 1           0 2026-02-09 21:12:29              0 #> 2           1 2026-02-09 21:12:29              1 #> 3           2 2026-02-09 21:12:29              2 #> 4           3 2026-02-09 21:12:29              3 #> 5           4 2026-02-09 21:12:29              4 #>                                                                           changes #> 1                                                           schemas_created, main #> 2                      tables_created, tables_inserted_into, main.vehicles_raw, 1 #> 3                    tables_created, tables_inserted_into, main.vehicles_clean, 2 #> 4                 tables_created, tables_inserted_into, main.vehicles_analysis, 3 #> 5 tables_created, tables_dropped, tables_inserted_into, main.vehicles_clean, 2, 4 #>          author                                           commit_message #> 1          <NA>                                                     <NA> #> 2 Data Engineer                         Initial load of raw vehicle data #> 3 Data Engineer                       Clean and standardize vehicle data #> 4  Data Analyst Create analysis-ready dataset with efficiency categories #> 5 Data Engineer                 Add gear type conversion to silver layer #>   commit_extra_info #> 1              <NA> #> 2              <NA> #> 3              <NA> #> 4              <NA> #> 5              <NA>  # Time travel: Query the silver layer as it existed at snapshot 2 (before updates) get_ducklake_table_version(\"vehicles_clean\", version = 2) |>   select(mpg, cyl, gear) |>   head(3) #> # Source:   SQL [?? x 3] #> # Database: DuckDB 1.4.4 [tgerke@Darwin 23.6.0:R 4.5.2//private/var/folders/b7/664jmq55319dcb7y4jdb39zr0000gq/T/RtmpOkTzSg/duckplyr/duckplyr2e58131c620d.duckdb] #>     mpg cyl    gear #>   <dbl> <chr> <dbl> #> 1  21   6.0       4 #> 2  21   6.0       4 #> 3  22.8 4.0       4  # Clean up detach_ducklake(\"my_data_lake\")"},{"path":"https://tgerke.github.io/ducklake-r/index.html","id":"medallion-architecture","dir":"","previous_headings":"","what":"Medallion architecture","title":"ducklake: Interact with DuckLake from R","text":"ducklake implements layered data architecture (medallion pattern) ensures data quality traceability: Bronze layer (raw): Data exactly received source systems—preserves original data audit trails Silver layer (cleaned): Standardized, cleaned data transformations validations—trusted source analysis Gold layer (analytics): Business-logic datasets optimized specific analyses, dashboards, reports layer automatically versioned, providing complete data lineage raw source analysis-ready datasets. approach enables: Complete audit trail: Original data preserved alongside transformations Reprocessability: Reprocess bronze cleaning logic changes without re-extracting source Data lineage: Clear progression raw → cleaned → analysis-ready Validation: Compare layers verify transformations Quality assurance: Separate concerns ingestion, cleaning, analysis","code":""},{"path":"https://tgerke.github.io/ducklake-r/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn more","title":"ducklake: Interact with DuckLake from R","text":"Check pkgdown site detailed vignettes: Clinical Trial Data Lake - Start : Complete workflow SDTM ADaM regulatory artifacts Cookbook - Quick recipes common operations Modifying Tables - Two approaches table modifications Transactions - ACID transaction support Time Travel - Query historical data","code":""},{"path":"https://tgerke.github.io/ducklake-r/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key features","title":"ducklake: Interact with DuckLake from R","text":"Versioned data lake: Every data change automatically tracked timestamps metadata Lightweight snapshots: Create unlimited snapshots without frequent compacting steps Medallion architecture: Bronze/silver/gold layers data lineage quality ACID transactions: Atomic updates concurrent access transactional guarantees multi-table operations Time travel: Query data exactly existed point time—essential reproducibility Performance-oriented: Uses Parquet columnar storage statistics filter pushdown, enabling fast queries large datasets Schema evolution: Adapt table schemas time requirements change Tidyverse interface: Familiar dplyr syntax data manipulation Two complementary approaches: rows_* functions data.frames pipeline functions dplyr workflows Complete audit trails: changed , , —suitable regulated industries Seamless integration: Works duckdb, duckplyr, broader R ecosystem","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/attach_ducklake.html","id":null,"dir":"Reference","previous_headings":"","what":"Create or attach a ducklake — attach_ducklake","title":"Create or attach a ducklake — attach_ducklake","text":"function wrapper ducklake ATTACH command. create new DuckDB-backed DuckLake specified name exist, connect existing DuckLake exist. connection stored package environment can closed detach_ducklake().","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/attach_ducklake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create or attach a ducklake — attach_ducklake","text":"","code":"attach_ducklake(ducklake_name, lake_path = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/attach_ducklake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create or attach a ducklake — attach_ducklake","text":"ducklake_name Name ducklake file, ducklake:{ducklake_name}.ducklake lake_path Optional directory path ducklake. specified, ducklake database file Parquet data files stored location. specified, ducklake created current working directory data files {ducklake_name}.ducklake.files.","code":""},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/reference/backup_ducklake.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a complete DuckLake backup — backup_ducklake","title":"Create a complete DuckLake backup — backup_ducklake","text":"Creates timestamped backup catalog database data files. backup includes complete state DuckLake time backup, allowing point--time recovery.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/backup_ducklake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a complete DuckLake backup — backup_ducklake","text":"","code":"backup_ducklake(ducklake_name, lake_path, backup_path)"},{"path":"https://tgerke.github.io/ducklake-r/reference/backup_ducklake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a complete DuckLake backup — backup_ducklake","text":"ducklake_name Name attached DuckLake lake_path Path DuckLake directory containing catalog file backup_path Directory backups stored. timestamped subdirectory created within path.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/backup_ducklake.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a complete DuckLake backup — backup_ducklake","text":"Invisibly returns path created backup directory","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/backup_ducklake.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a complete DuckLake backup — backup_ducklake","text":"function creates complete backup : Creating timestamped backup directory Copying catalog database file (.ducklake) Copying data files main/ directory Important notes: Transactions committed backup tracked recovering. data exist Parquet files, backup point earlier snapshot. Consider coordinating backups maintenance operations (compaction cleanup) optimal storage efficiency. production systems, schedule backups using {cronR} {taskscheduleR}.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/backup_ducklake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a complete DuckLake backup — backup_ducklake","text":"","code":"if (FALSE) { # \\dontrun{ # Create a DuckLake lake_dir <- tempfile(\"my_lake\") dir.create(lake_dir) attach_ducklake(\"my_lake\", lake_path = lake_dir)  # Add some data with_transaction(   create_table(mtcars, \"cars\"),   author = \"User\",   commit_message = \"Initial data\" )  # Create a backup backup_dir <- backup_ducklake(   ducklake_name = \"my_lake\",   lake_path = lake_dir,   backup_path = file.path(lake_dir, \"backups\") )  # To restore from backup: # detach_ducklake(\"my_lake\") # attach_ducklake(\"my_lake\", lake_path = backup_dir) } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/begin_transaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Begin a transaction — begin_transaction","title":"Begin a transaction — begin_transaction","text":"Starts new transaction DuckDB connection. subsequent operations part transaction committed rolled back.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/begin_transaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Begin a transaction — begin_transaction","text":"","code":"begin_transaction(conn = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/begin_transaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Begin a transaction — begin_transaction","text":"conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/begin_transaction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Begin a transaction — begin_transaction","text":"Invisibly returns TRUE success","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/begin_transaction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Begin a transaction — begin_transaction","text":"Transactions allow group multiple operations together ensure either succeed fail. Use commit_transaction() apply changes rollback_transaction() discard . DuckDB supports full ACID transactions multiple isolation levels.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/begin_transaction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Begin a transaction — begin_transaction","text":"","code":"if (FALSE) { # \\dontrun{ # Start a transaction begin_transaction()  # Make some changes get_ducklake_table(\"my_table\") |>   filter(status == \"pending\") |>   mutate(status = \"processed\") |>   ducklake_exec()  # Commit if everything looks good commit_transaction()  # Or rollback if something went wrong # rollback_transaction() } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/commit_transaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Commit a transaction — commit_transaction","title":"Commit a transaction — commit_transaction","text":"Commits current transaction, making changes permanent. Optionally adds metadata (author, commit message, extra info) snapshot.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/commit_transaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Commit a transaction — commit_transaction","text":"","code":"commit_transaction(   conn = NULL,   author = NULL,   commit_message = NULL,   commit_extra_info = NULL )"},{"path":"https://tgerke.github.io/ducklake-r/reference/commit_transaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Commit a transaction — commit_transaction","text":"conn Optional DuckDB connection object. provided, uses default ducklake connection. author Optional author name associate snapshot commit_message Optional commit message describing changes commit_extra_info Optional extra information commit","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/commit_transaction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Commit a transaction — commit_transaction","text":"Invisibly returns TRUE success","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/commit_transaction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Commit a transaction — commit_transaction","text":"function commits changes made since begin_transaction() called, making permanent database. DuckLake automatically tracks changes ducklake_snapshot_changes metadata table. author, commit_message, commit_extra_info provided, automatically added snapshot metadata committing.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/commit_transaction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Commit a transaction — commit_transaction","text":"","code":"if (FALSE) { # \\dontrun{ # Basic commit begin_transaction() # ... make changes ... commit_transaction()  # Commit with metadata begin_transaction() create_table(mtcars, \"cars\") commit_transaction(   author = \"John Doe\",   commit_message = \"Add cars dataset\" ) } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/create_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a DuckLake table — create_table","title":"Create a DuckLake table — create_table","text":"Create DuckLake table","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/create_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a DuckLake table — create_table","text":"","code":"create_table(data_source, table_name)"},{"path":"https://tgerke.github.io/ducklake-r/reference/create_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a DuckLake table — create_table","text":"data_source Raw data source. Can : URL (http:// https://) file path (e.g., \"data.csv\", \"data.parquet\") R data.frame tibble lazy table (tbl_duckdb_connection tbl_lazy) table_name Name new table","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/create_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a DuckLake table — create_table","text":"","code":"if (FALSE) { # \\dontrun{ # From URL create_table(\"https://example.com/data.csv\", \"my_table\")  # From local file create_table(\"data.csv\", \"my_table\")  # From data.frame create_table(mtcars, \"my_table\")  # From lazy table (pipe-friendly) get_ducklake_table(\"source_table\") %>%    filter(x > 5) %>%   create_table(\"filtered_table\") } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/detach_ducklake.html","id":null,"dir":"Reference","previous_headings":"","what":"Detach from a ducklake — detach_ducklake","title":"Detach from a ducklake — detach_ducklake","text":"Closes DuckDB connection detaches current DuckLake.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/detach_ducklake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detach from a ducklake — detach_ducklake","text":"","code":"detach_ducklake(ducklake_name = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/detach_ducklake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detach from a ducklake — detach_ducklake","text":"ducklake_name Optional name ducklake detach. provided, closes current connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/detach_ducklake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detach from a ducklake — detach_ducklake","text":"","code":"if (FALSE) { # \\dontrun{ attach_ducklake(\"my_ducklake\") # ... do work ... detach_ducklake(\"my_ducklake\") } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/ducklake_exec.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute DuckLake operations from dplyr queries — ducklake_exec","title":"Execute DuckLake operations from dplyr queries — ducklake_exec","text":"Execute DuckLake operations dplyr queries","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/ducklake_exec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute DuckLake operations from dplyr queries — ducklake_exec","text":"","code":"ducklake_exec(.data, table_name = NULL, .quiet = TRUE)"},{"path":"https://tgerke.github.io/ducklake-r/reference/ducklake_exec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute DuckLake operations from dplyr queries — ducklake_exec","text":".data dplyr query object (tbl_lazy) accumulated operations table_name target table name operation. provided, extracted table attribute (set get_ducklake_table()) .quiet Logical, whether suppress debug output (default TRUE)","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/ducklake_exec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute DuckLake operations from dplyr queries — ducklake_exec","text":"result duckplyr::db_exec()","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/ducklake_exec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Execute DuckLake operations from dplyr queries — ducklake_exec","text":"function automatically detects type operation based dplyr verbs: Filter-queries generate DELETE operations (removes rows match filter) Queries mutate() generate UPDATE operations queries generate INSERT operations","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/ducklake_exec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute DuckLake operations from dplyr queries — ducklake_exec","text":"","code":"if (FALSE) { # \\dontrun{ # Delete rows that don't match filter (table name inferred) get_ducklake_table(\"my_table\") |>   filter(status == \"inactive\") |>   ducklake_exec()  # Update specific rows (table name inferred) get_ducklake_table(\"my_table\") |>   filter(id == 123) |>   mutate(status = \"updated\") |>   ducklake_exec()  # Or provide table name explicitly tbl(con, \"my_table\") |>   select(id, name) |>   mutate(computed_field = name * 2) |>   ducklake_exec(\"my_table\") } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/extract_assignments_from_sql.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract column assignments from SQL SELECT statement — extract_assignments_from_sql","title":"Extract column assignments from SQL SELECT statement — extract_assignments_from_sql","text":"Extract column assignments SQL SELECT statement","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/extract_assignments_from_sql.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract column assignments from SQL SELECT statement — extract_assignments_from_sql","text":"","code":"extract_assignments_from_sql(sql_text)"},{"path":"https://tgerke.github.io/ducklake-r/reference/extract_assignments_from_sql.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract column assignments from SQL SELECT statement — extract_assignments_from_sql","text":"sql_text SQL SELECT statement","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/extract_assignments_from_sql.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract column assignments from SQL SELECT statement — extract_assignments_from_sql","text":"string comma-separated column assignments UPDATE SET clause","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the current DuckLake connection — get_ducklake_connection","title":"Get the current DuckLake connection — get_ducklake_connection","text":"function retrieves active DuckLake connection. connection explicitly set via set_ducklake_connection(), falls back duckplyr's default DuckDB connection seamless integration.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the current DuckLake connection — get_ducklake_connection","text":"","code":"get_ducklake_connection()"},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the current DuckLake connection — get_ducklake_connection","text":"DuckDB connection object","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_connection.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Get the current DuckLake connection — get_ducklake_connection","text":"function uses duckplyr:::get_default_duckdb_connection() fallback connection explicitly set. accesses unexported function, necessary proper duckplyr integration duckplyr's connection provides critical setup (singleton pattern, temp directory configuration, R function loading, macro registration) easily replicated. See duckplyr source details: https://github.com/tidyverse/duckplyr/blob/main/R/relational-duckdb.R","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a DuckLake table — get_ducklake_table","title":"Get a DuckLake table — get_ducklake_table","text":"Get DuckLake table","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a DuckLake table — get_ducklake_table","text":"","code":"get_ducklake_table(tbl_name)"},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a DuckLake table — get_ducklake_table","text":"tbl_name Character string, name table retrieve","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a DuckLake table — get_ducklake_table","text":"DuckLake table class tbl_duckdb_connection table name stored attribute","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_asof.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a table at a specific timestamp (time travel) — get_ducklake_table_asof","title":"Query a table at a specific timestamp (time travel) — get_ducklake_table_asof","text":"Retrieves data DuckLake table existed specific point time using DuckLake's (TIMESTAMP => ...) syntax.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_asof.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a table at a specific timestamp (time travel) — get_ducklake_table_asof","text":"","code":"get_ducklake_table_asof(table_name, timestamp, conn = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_asof.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a table at a specific timestamp (time travel) — get_ducklake_table_asof","text":"table_name name table query timestamp POSIXct timestamp character string ISO 8601 format (e.g., \"2024-01-15 10:30:00\") conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_asof.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query a table at a specific timestamp (time travel) — get_ducklake_table_asof","text":"dplyr lazy query object (tbl_lazy) can manipulated dplyr verbs","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_asof.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query a table at a specific timestamp (time travel) — get_ducklake_table_asof","text":"DuckLake supports time-travel queries, allowing query historical data existed specific timestamp. uses syntax: SELECT * table (TIMESTAMP => 'timestamp') useful : Auditing changes time Recovering accidentally deleted modified data Comparing data states across different time points Regulatory compliance data lineage documentation timestamp must within range available snapshots table. Use list_table_snapshots() see available snapshot times. Important: querying snapshot's exact timestamp, may need add small time buffer (e.g., +1 second) ensure snapshot found. time-travel query looks snapshots created specified timestamp.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_asof.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query a table at a specific timestamp (time travel) — get_ducklake_table_asof","text":"","code":"if (FALSE) { # \\dontrun{ # Query data as it existed yesterday yesterday <- Sys.time() - (24 * 60 * 60) get_ducklake_table_asof(\"my_table\", yesterday) |>   filter(category == \"A\") |>   collect()  # Query data at a specific snapshot time snapshots <- list_table_snapshots(\"my_table\") # Add 1 second to ensure the snapshot is found get_ducklake_table_asof(\"my_table\", snapshots$snapshot_time[2] + 1) |>   summarise(total = sum(amount)) } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a table at a specific version/snapshot — get_ducklake_table_version","title":"Query a table at a specific version/snapshot — get_ducklake_table_version","text":"Retrieves data DuckLake table specific snapshot ID using DuckLake's (VERSION => ...) syntax.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a table at a specific version/snapshot — get_ducklake_table_version","text":"","code":"get_ducklake_table_version(table_name, version, conn = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a table at a specific version/snapshot — get_ducklake_table_version","text":"table_name name table query version snapshot_id query (get list_table_snapshots()) conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_version.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query a table at a specific version/snapshot — get_ducklake_table_version","text":"dplyr lazy query object (tbl_lazy) can manipulated dplyr verbs","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_version.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query a table at a specific version/snapshot — get_ducklake_table_version","text":"function allows query specific snapshot table using snapshot_id. uses syntax: SELECT * table (VERSION => snapshot_id) time create modify table within transaction, DuckLake creates new snapshot unique snapshot_id. Note snapshot_id schema_version typically value - represent snapshot identifier. Use list_table_snapshots(table_name) see available snapshots IDs.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_ducklake_table_version.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query a table at a specific version/snapshot — get_ducklake_table_version","text":"","code":"if (FALSE) { # \\dontrun{ # Get available snapshots snapshots <- list_table_snapshots(\"my_table\")  # Query the first snapshot version get_ducklake_table_version(\"my_table\", snapshots$snapshot_id[1]) |>   filter(status == \"active\") |>   collect() } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/get_metadata_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a DuckLake metadata table — get_metadata_table","title":"Get a DuckLake metadata table — get_metadata_table","text":"Get DuckLake metadata table","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_metadata_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a DuckLake metadata table — get_metadata_table","text":"","code":"get_metadata_table(tbl_name, ducklake_name = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/get_metadata_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a DuckLake metadata table — get_metadata_table","text":"tbl_name Character string, name table retrieve ducklake_name Character string, name ducklake database (optional, defaults current active ducklake)","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/get_metadata_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a DuckLake metadata table — get_metadata_table","text":"DuckLake table class tbl_duckdb_connection","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/install_ducklake.html","id":null,"dir":"Reference","previous_headings":"","what":"Install the ducklake extension to duckdb — install_ducklake","title":"Install the ducklake extension to duckdb — install_ducklake","text":"needs run system.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/install_ducklake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install the ducklake extension to duckdb — install_ducklake","text":"","code":"install_ducklake()"},{"path":"https://tgerke.github.io/ducklake-r/reference/install_ducklake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Install the ducklake extension to duckdb — install_ducklake","text":"","code":"ducklake::install_ducklake()"},{"path":"https://tgerke.github.io/ducklake-r/reference/list_table_snapshots.html","id":null,"dir":"Reference","previous_headings":"","what":"List available snapshots for a table — list_table_snapshots","title":"List available snapshots for a table — list_table_snapshots","text":"Retrieves information available snapshots/versions table.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/list_table_snapshots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available snapshots for a table — list_table_snapshots","text":"","code":"list_table_snapshots(table_name = NULL, ducklake_name = NULL, conn = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/list_table_snapshots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List available snapshots for a table — list_table_snapshots","text":"table_name name table query ducklake_name name ducklake (database) query. NULL, attempt infer current database. conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/list_table_snapshots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List available snapshots for a table — list_table_snapshots","text":"data frame snapshot information (version, timestamp, etc.)","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/list_table_snapshots.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List available snapshots for a table — list_table_snapshots","text":"function queries snapshot history table, showing available versions timestamps. useful understanding historical versions available time-travel queries. Note: exact format availability information depends table format (Delta Lake, Iceberg, etc.).","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/list_table_snapshots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List available snapshots for a table — list_table_snapshots","text":"","code":"if (FALSE) { # \\dontrun{ # List all snapshots for a table list_table_snapshots(\"my_table\") } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/replace_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace a table with modified data and create a new snapshot — replace_table","title":"Replace a table with modified data and create a new snapshot — replace_table","text":"Replace table modified data create new snapshot","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/replace_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace a table with modified data and create a new snapshot — replace_table","text":"","code":"replace_table(.data, table_name, .quiet = TRUE)"},{"path":"https://tgerke.github.io/ducklake-r/reference/replace_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace a table with modified data and create a new snapshot — replace_table","text":".data dplyr query object (tbl_lazy) transformations table_name Table name replace .quiet Logical, whether suppress messages (default TRUE)","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/replace_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace a table with modified data and create a new snapshot — replace_table","text":"Invisibly returns NULL","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/replace_table.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Replace a table with modified data and create a new snapshot — replace_table","text":"function designed schema changes bulk transformations create new versioned snapshot. : Collects transformed data Drops existing table Creates new table updated schema/data operations happen within current transaction context. Use begin_transaction() commit_transaction() ensure proper versioning. use replace_table(): Adding new columns - DuckLake UPDATE add columns; use replace_table() Removing columns - Restructure schema select() Versioning needed - Creates snapshots via DROP + CREATE time travel Complex transformations - Apply full dplyr pipelines naturally use update_table() instead: Modifying existing column values (schema changes) Performance critical versioning needed Making targeted corrections specific rows","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/replace_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace a table with modified data and create a new snapshot — replace_table","text":"","code":"if (FALSE) { # \\dontrun{ # Add new derived columns with versioning begin_transaction() get_ducklake_table(\"adsl\") |>   mutate(     AGE65FL = if_else(AGE >= 65, \"Y\", \"N\"),     AGECAT = case_when(       AGE < 65 ~ \"<65\",       AGE >= 65 & AGE < 75 ~ \"65-74\",       AGE >= 75 ~ \">=75\"     )   ) |>   replace_table(\"adsl\") commit_transaction()  # Remove columns and create new snapshot begin_transaction() get_ducklake_table(\"adsl\") |>   select(-AGE65FL, -AGECAT) |>   replace_table(\"adsl\") commit_transaction() } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/restore_table_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore a table to a previous version — restore_table_version","title":"Restore a table to a previous version — restore_table_version","text":"Restores table specific version timestamp, reverting changes made point.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/restore_table_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore a table to a previous version — restore_table_version","text":"","code":"restore_table_version(   table_name,   version = NULL,   timestamp = NULL,   conn = NULL )"},{"path":"https://tgerke.github.io/ducklake-r/reference/restore_table_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore a table to a previous version — restore_table_version","text":"table_name name table restore version Optional version number restore timestamp Optional timestamp restore (POSIXct character) conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/restore_table_version.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Restore a table to a previous version — restore_table_version","text":"Invisibly returns TRUE success","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/restore_table_version.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Restore a table to a previous version — restore_table_version","text":"function restores table previous state. must specify either version timestamp, . WARNING: operation modifies table easily undone. Consider using within transaction backing data first.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/restore_table_version.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Restore a table to a previous version — restore_table_version","text":"","code":"if (FALSE) { # \\dontrun{ # Restore to version 5 restore_table_version(\"my_table\", version = 5)  # Restore to a specific timestamp restore_table_version(\"my_table\", timestamp = \"2024-01-15 10:00:00\") } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/rollback_transaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Rollback a transaction — rollback_transaction","title":"Rollback a transaction — rollback_transaction","text":"Rolls back current transaction, discarding changes made since transaction began.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rollback_transaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rollback a transaction — rollback_transaction","text":"","code":"rollback_transaction(conn = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/rollback_transaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rollback a transaction — rollback_transaction","text":"conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rollback_transaction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rollback a transaction — rollback_transaction","text":"Invisibly returns TRUE success","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rollback_transaction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rollback a transaction — rollback_transaction","text":"function discards changes made since begin_transaction() called, reverting database state transaction began.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rollback_transaction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rollback a transaction — rollback_transaction","text":"","code":"if (FALSE) { # \\dontrun{ begin_transaction() # ... make changes ... # Something went wrong, rollback rollback_transaction() } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete rows from a DuckLake table — rows_delete","title":"Delete rows from a DuckLake table — rows_delete","text":"wrapper around dplyr::rows_delete() in_place = TRUE default, since DuckLake designed -place modifications.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete rows from a DuckLake table — rows_delete","text":"","code":"rows_delete(   x,   y,   by = NULL,   copy = TRUE,   in_place = TRUE,   unmatched = \"ignore\",   ... )"},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete rows from a DuckLake table — rows_delete","text":"x Target table (get_ducklake_table()) y Data frame rows delete (matched '' columns) Column(s) match copy Whether copy y source x (default TRUE) in_place Whether modify table place (default TRUE DuckLake) unmatched handle unmatched rows (default \"error\") ... Additional arguments passed dplyr::rows_delete()","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_delete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delete rows from a DuckLake table — rows_delete","text":"updated table","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_delete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delete rows from a DuckLake table — rows_delete","text":"","code":"if (FALSE) { # \\dontrun{ rows_delete(   get_ducklake_table(\"my_table\"),   data.frame(id = c(1, 2, 3)),   by = \"id\" ) } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_insert.html","id":null,"dir":"Reference","previous_headings":"","what":"Insert rows into a DuckLake table — rows_insert","title":"Insert rows into a DuckLake table — rows_insert","text":"wrapper around dplyr::rows_insert() in_place = TRUE default, since DuckLake designed -place modifications.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_insert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Insert rows into a DuckLake table — rows_insert","text":"","code":"rows_insert(   x,   y,   by = NULL,   copy = TRUE,   in_place = TRUE,   conflict = \"ignore\",   ... )"},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_insert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Insert rows into a DuckLake table — rows_insert","text":"x Target table (get_ducklake_table()) y Data frame new rows Column(s) match (conflict detection) copy Whether copy y source x (default TRUE) in_place Whether modify table place (default TRUE DuckLake) conflict handle conflicts (default \"error\") ... Additional arguments passed dplyr::rows_insert()","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_insert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Insert rows into a DuckLake table — rows_insert","text":"updated table","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_insert.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Insert rows into a DuckLake table — rows_insert","text":"","code":"if (FALSE) { # \\dontrun{ rows_insert(   get_ducklake_table(\"my_table\"),   data.frame(id = 99, value = \"new row\"),   by = \"id\" ) } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update rows in a DuckLake table — rows_update","title":"Update rows in a DuckLake table — rows_update","text":"wrapper around dplyr::rows_update() in_place = TRUE default, since DuckLake designed -place modifications.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update rows in a DuckLake table — rows_update","text":"","code":"rows_update(   x,   y,   by = NULL,   copy = TRUE,   in_place = TRUE,   unmatched = \"ignore\",   ... )"},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update rows in a DuckLake table — rows_update","text":"x Target table (get_ducklake_table()) y Data frame updates Column(s) match copy Whether copy y source x (default TRUE) in_place Whether modify table place (default TRUE DuckLake) unmatched handle unmatched rows (default \"error\") ... Additional arguments passed dplyr::rows_update()","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_update.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update rows in a DuckLake table — rows_update","text":"updated table","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/rows_update.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update rows in a DuckLake table — rows_update","text":"","code":"if (FALSE) { # \\dontrun{ # Update rows - in_place = TRUE by default rows_update(   get_ducklake_table(\"my_table\"),   data.frame(id = 1, value = \"new\"),   by = \"id\" ) } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/set_ducklake_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the DuckLake connection — set_ducklake_connection","title":"Set the DuckLake connection — set_ducklake_connection","text":"Set DuckLake connection","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/set_ducklake_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the DuckLake connection — set_ducklake_connection","text":"","code":"set_ducklake_connection(conn)"},{"path":"https://tgerke.github.io/ducklake-r/reference/set_ducklake_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the DuckLake connection — set_ducklake_connection","text":"conn DuckDB connection object","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/set_snapshot_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Set metadata for the most recent snapshot — set_snapshot_metadata","title":"Set metadata for the most recent snapshot — set_snapshot_metadata","text":"Updates author, commit message, /extra info recent snapshot DuckLake catalog.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/set_snapshot_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set metadata for the most recent snapshot — set_snapshot_metadata","text":"","code":"set_snapshot_metadata(   ducklake_name,   author = NULL,   commit_message = NULL,   commit_extra_info = NULL,   conn = NULL )"},{"path":"https://tgerke.github.io/ducklake-r/reference/set_snapshot_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set metadata for the most recent snapshot — set_snapshot_metadata","text":"ducklake_name name DuckLake catalog author Optional author name associate snapshot commit_message Optional commit message describing changes commit_extra_info Optional extra information commit conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/set_snapshot_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set metadata for the most recent snapshot — set_snapshot_metadata","text":"Invisibly returns TRUE success","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/set_snapshot_metadata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set metadata for the most recent snapshot — set_snapshot_metadata","text":"function updates metadata columns ducklake_snapshot_changes table recent snapshot. Call commit_transaction() add audit information commits.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/set_snapshot_metadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set metadata for the most recent snapshot — set_snapshot_metadata","text":"","code":"if (FALSE) { # \\dontrun{ begin_transaction() # ... make changes ... commit_transaction()  # Add metadata to the snapshot set_snapshot_metadata(   ducklake_name = \"my_ducklake\",   author = \"Data Team\",   commit_message = \"Updated station names for clarity\" ) } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/show_ducklake_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Show the SQL that would be executed by ducklake operations — show_ducklake_query","title":"Show the SQL that would be executed by ducklake operations — show_ducklake_query","text":"function shows SQL generated executed ducklake. useful debugging understanding SQL sent DuckDB.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/show_ducklake_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show the SQL that would be executed by ducklake operations — show_ducklake_query","text":"","code":"show_ducklake_query(.data, table_name = NULL)"},{"path":"https://tgerke.github.io/ducklake-r/reference/show_ducklake_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show the SQL that would be executed by ducklake operations — show_ducklake_query","text":".data dplyr query object (tbl_lazy) table_name target table name operation. provided, extracted table attribute (set get_ducklake_table())","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/show_ducklake_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show the SQL that would be executed by ducklake operations — show_ducklake_query","text":"first argument, invisibly (following show_query convention)","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/show_ducklake_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show the SQL that would be executed by ducklake operations — show_ducklake_query","text":"","code":"if (FALSE) { # \\dontrun{ # Show SQL for an update operation (table name inferred) get_ducklake_table(\"my_table\") |>   mutate(status = \"updated\") |>   show_ducklake_query() } # }"},{"path":"https://tgerke.github.io/ducklake-r/reference/with_transaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute code within a transaction — with_transaction","title":"Execute code within a transaction — with_transaction","text":"Wraps code execution transaction, automatically committing success rolling back error. provides R-idiomatic safer way handle transactions compared manually calling begin_transaction() commit_transaction().","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/with_transaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute code within a transaction — with_transaction","text":"","code":"with_transaction(   expr,   author = NULL,   commit_message = NULL,   commit_extra_info = NULL,   conn = NULL )"},{"path":"https://tgerke.github.io/ducklake-r/reference/with_transaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute code within a transaction — with_transaction","text":"expr R expression code block execute within transaction. Can single statement {...} block containing multiple statements. author Optional author name associate snapshot commit_message Optional commit message describing changes commit_extra_info Optional extra information commit conn Optional DuckDB connection object. provided, uses default ducklake connection.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/with_transaction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute code within a transaction — with_transaction","text":"Invisibly returns result expression","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/with_transaction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Execute code within a transaction — with_transaction","text":"function provides automatic error handling cleanup transactions: Begins transaction executing code Executes provided expression success: commits transaction adds metadata (provided) error: automatically rolls back transaction re-throws error pattern similar withr::with_*() functions provides better safety guarantees manually managing transactions.","code":""},{"path":"https://tgerke.github.io/ducklake-r/reference/with_transaction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute code within a transaction — with_transaction","text":"","code":"if (FALSE) { # \\dontrun{ # Single operation with_transaction(   create_table(mtcars, \"cars\"),   author = \"Data Team\",   commit_message = \"Add cars dataset\" )  # Multiple operations in a block with_transaction({   create_table(mtcars, \"cars\")   create_table(iris, \"flowers\") }, author = \"Data Team\", commit_message = \"Add datasets\")  # With dplyr pipeline with_transaction(   get_ducklake_table(\"cars\") |>     mutate(kpl = mpg * 0.425144) |>     replace_table(\"cars\"),   author = \"Data Team\",   commit_message = \"Add km/L column\" )  # Automatic rollback on error tryCatch(   with_transaction({     create_table(mtcars, \"cars\")     stop(\"Simulated error\")  # Transaction will be rolled back   }),   error = function(e) message(\"Transaction was rolled back: \", e$message) ) } # }"},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"ducklake-010","dir":"Changelog","previous_headings":"","what":"ducklake 0.1.0","title":"ducklake 0.1.0","text":"Initial release ducklake, R package versioned data lake infrastructure built DuckDB DuckLake.","code":""},{"path":[]},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"core-table-operations-0-1-0","dir":"Changelog","previous_headings":"Features","what":"Core Table Operations","title":"ducklake 0.1.0","text":"create_table() - Create new tables data lake get_ducklake_table() - Retrieve tables tibbles replace_table() - Replace entire table contents versioning","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"row-level-operations-0-1-0","dir":"Changelog","previous_headings":"Features","what":"Row-Level Operations","title":"ducklake 0.1.0","text":"rows_insert() - Insert new rows automatic versioning rows_update() - Update existing rows audit trail rows_delete() - Delete rows maintaining history","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"acid-transactions-0-1-0","dir":"Changelog","previous_headings":"Features","what":"ACID Transactions","title":"ducklake 0.1.0","text":"with_transaction() - Execute code blocks within transactions begin_transaction(), commit_transaction(), rollback_transaction() - Manual transaction control Full ACID compliance data integrity","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"time-travel-0-1-0","dir":"Changelog","previous_headings":"Features","what":"Time Travel","title":"ducklake 0.1.0","text":"get_ducklake_table_asof() - Query table state specific timestamps get_ducklake_table_version() - Retrieve specific table versions list_table_snapshots() - View complete version history restore_table_version() - Roll back previous versions","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"metadata-and-audit-trail-0-1-0","dir":"Changelog","previous_headings":"Features","what":"Metadata and Audit Trail","title":"ducklake 0.1.0","text":"get_metadata_table() - Access comprehensive metadata set_snapshot_metadata() - Add author, commit messages, tags Complete lineage tracking data changes","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"connection-management-0-1-0","dir":"Changelog","previous_headings":"Features","what":"Connection Management","title":"ducklake 0.1.0","text":"install_ducklake() - Install/update DuckLake extension attach_ducklake() - Initialize data lake connections detach_ducklake() - Clean connections get_ducklake_connection(), set_ducklake_connection() - Manage active connections","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"query-execution-0-1-0","dir":"Changelog","previous_headings":"Features","what":"Query Execution","title":"ducklake 0.1.0","text":"ducklake_exec() - Execute SQL automatic assignment handling show_ducklake_query() - Preview translated SQL queries extract_assignments_from_sql() - Parse SQL table assignments","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"backup-and-maintenance-0-1-0","dir":"Changelog","previous_headings":"Features","what":"Backup and Maintenance","title":"ducklake 0.1.0","text":"backup_ducklake() - Create incremental backups Support local remote backup locations","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"vignettes-0-1-0","dir":"Changelog","previous_headings":"","what":"Vignettes","title":"ducklake 0.1.0","text":"Getting Started - Introduction ducklake workflows Clinical Trial Data Lake - Industry-specific use case Modifying Tables - Comprehensive guide row operations Working Transactions - ACID transaction patterns Time Travel Queries - Historical data access Storage Backup Management - Data persistence strategies","code":""},{"path":"https://tgerke.github.io/ducklake-r/news/index.html","id":"lifecycle-0-1-0","dir":"Changelog","previous_headings":"","what":"Lifecycle","title":"ducklake 0.1.0","text":"package currently experimental status. API may change gather feedback early users, core functionality stable ready pilot projects.","code":""}]
